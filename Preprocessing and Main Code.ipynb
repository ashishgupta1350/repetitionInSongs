{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"mergedData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>4385.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>2009.03626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>32.88715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>2011.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>2013.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>2016.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>2019.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            years\n",
       "count  4385.00000\n",
       "mean   2009.03626\n",
       "std      32.88715\n",
       "min       0.00000\n",
       "25%    2011.00000\n",
       "50%    2013.00000\n",
       "75%    2016.00000\n",
       "max    2019.00000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieName</th>\n",
       "      <th>songName</th>\n",
       "      <th>songSinger</th>\n",
       "      <th>songMusic</th>\n",
       "      <th>songLyricist</th>\n",
       "      <th>songLyrics</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Bala</td>\n",
       "      <td>Don’t Be Shy</td>\n",
       "      <td>Badshah, Shalmali Kholgade, Gurdeep Mehendi</td>\n",
       "      <td>Sachin-Jigar</td>\n",
       "      <td>Mellow D, Badshah</td>\n",
       "      <td>Sun, main hoon thoda sanki\\r\\nKarun mann ki\\r\\...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Laal Kaptaan</td>\n",
       "      <td>Lahu Ka Rang Kara</td>\n",
       "      <td>Samira Koppikar</td>\n",
       "      <td>Samira Koppikar</td>\n",
       "      <td>Sahib</td>\n",
       "      <td>Morey.. lahu ka rang kara\\r\\nMorey lahu ka ran...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Laal Kaptaan</td>\n",
       "      <td>Red Red Najariya</td>\n",
       "      <td>Shreya Ghoshal</td>\n",
       "      <td>Samira Koppikar</td>\n",
       "      <td>Saurabh Jain</td>\n",
       "      <td>Badnaam shehar, badnaam gully\\r\\nIsme har raat...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Laal Kaptaan</td>\n",
       "      <td>Kaal Kaal</td>\n",
       "      <td>Brijesh Shandilya, Dino James</td>\n",
       "      <td>Samira Koppikar</td>\n",
       "      <td>Saurabh Jain</td>\n",
       "      <td>Kaal kaal, kaal kaal, jo sapaat chal raha\\r\\nW...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Laal Kaptaan</td>\n",
       "      <td>Taandav</td>\n",
       "      <td>Kailash Kher, Brijesh Shandilya</td>\n",
       "      <td>Samira Koppikar</td>\n",
       "      <td>Puneet Sharma</td>\n",
       "      <td>Shor hai andher mein\\r\\nJo dher murda pedon ka...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      movieName           songName  \\\n",
       "0          Bala       Don’t Be Shy   \n",
       "1  Laal Kaptaan  Lahu Ka Rang Kara   \n",
       "2  Laal Kaptaan   Red Red Najariya   \n",
       "3  Laal Kaptaan          Kaal Kaal   \n",
       "4  Laal Kaptaan            Taandav   \n",
       "\n",
       "                                    songSinger        songMusic  \\\n",
       "0  Badshah, Shalmali Kholgade, Gurdeep Mehendi     Sachin-Jigar   \n",
       "1                              Samira Koppikar  Samira Koppikar   \n",
       "2                               Shreya Ghoshal  Samira Koppikar   \n",
       "3                Brijesh Shandilya, Dino James  Samira Koppikar   \n",
       "4              Kailash Kher, Brijesh Shandilya  Samira Koppikar   \n",
       "\n",
       "        songLyricist                                         songLyrics  years  \n",
       "0  Mellow D, Badshah  Sun, main hoon thoda sanki\\r\\nKarun mann ki\\r\\...   2019  \n",
       "1              Sahib  Morey.. lahu ka rang kara\\r\\nMorey lahu ka ran...   2019  \n",
       "2       Saurabh Jain  Badnaam shehar, badnaam gully\\r\\nIsme har raat...   2019  \n",
       "3       Saurabh Jain  Kaal kaal, kaal kaal, jo sapaat chal raha\\r\\nW...   2019  \n",
       "4      Puneet Sharma  Shor hai andher mein\\r\\nJo dher murda pedon ka...   2019  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "dataset = ''\n",
    "countOfWords = 0\n",
    "uniqueWords = {}\n",
    "for song in list(df.songLyrics):\n",
    "    listOfWords = re.split(r'[;,\\s...\\n()\\'!?.]\\s*',song) # gets me a list of words\n",
    "    for word in listOfWords:\n",
    "#         if word ==  '' || word=='(' || word==')' || word =='\\'':\n",
    "#             pass\n",
    "#         else:\n",
    "        word = word.lower()\n",
    "        if word == 'x2':\n",
    "            continue\n",
    "        if word == 'x4':\n",
    "            continue\n",
    "        dataset+=' '+word\n",
    "        countOfWords+=1\n",
    "        uniqueWords[word] = 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> \n",
      " -> \n",
      "\n",
      "saanu -> sa\n",
      "\n",
      "kare -> kar\n",
      "\n",
      "ishaare -> ishaar\n",
      "ishaare doesnot exist\n",
      "\n",
      "touch -> touch\n",
      "touch -> touch\n",
      "\n",
      "my -> my\n",
      "my -> my\n",
      "\n",
      "don -> do\n",
      "\n",
      "t -> t\n",
      "t -> t\n",
      "\n",
      "be -> be\n",
      "be -> be\n",
      "\n",
      "shy -> sh\n",
      "\n",
      "honey -> ho\n",
      "\n",
      "fly -> fl\n",
      "fly doesnot exist\n",
      "\n",
      "befikar -> befikar\n",
      "befikar -> befikar\n",
      "\n",
      "kuch -> kuch\n",
      "kuch -> kuch\n",
      "\n",
      "bole -> bol\n",
      "\n",
      "bina -> bi\n",
      "\n",
      "aankhon -> aankh\n",
      "\n",
      "se -> se\n",
      "se -> se\n",
      "\n",
      "shaitani -> shait\n",
      "shaitani doesnot exist\n",
      "\n",
      "jaan -> ja\n",
      "\n",
      "bujhke -> bujhk\n",
      "bujhke doesnot exist\n",
      "\n",
      "mere -> mer\n",
      "\n",
      "saath -> saath\n",
      "saath -> saath\n",
      "\n",
      "tujhko -> tujhk\n",
      "tujhko doesnot exist\n",
      "\n",
      "dekhe -> dekh\n",
      "\n",
      "mera -> mer\n",
      "\n",
      "ab -> ab\n",
      "ab -> ab\n",
      "\n",
      "kahin -> kah\n",
      "\n",
      "lage -> la\n",
      "\n",
      "husn -> hus\n",
      "husn doesnot exist\n",
      "\n",
      "pe -> pe\n",
      "pe -> pe\n",
      "\n",
      "aankhein -> aankh\n",
      "\n",
      "sek -> sek\n",
      "sek -> sek\n",
      "\n",
      "kar -> kar\n",
      "kar -> kar\n",
      "\n",
      "thake -> thak\n",
      "\n",
      "upar -> upar\n",
      "upar -> upar\n",
      "\n",
      "raat -> raat\n",
      "raat -> raat\n",
      "\n",
      "hoti -> hot\n",
      "\n",
      "ja -> ja\n",
      "ja -> ja\n",
      "\n",
      "rahi -> rah\n",
      "\n",
      "naughty -> naught\n",
      "naughty doesnot exist\n",
      "\n",
      "morey -> mor\n",
      "morey -> mor\n",
      "\n",
      "lahu -> lah\n",
      "\n",
      "ka -> ka\n",
      "ka -> ka\n",
      "\n",
      "rang -> ra\n",
      "\n",
      "kara -> kar\n",
      "\n",
      "kaare -> kaar\n",
      "\n",
      "badarwa -> badarw\n",
      "badarwa doesnot exist\n",
      "\n",
      "manwa -> manw\n",
      "manwa doesnot exist\n",
      "\n",
      "ghoome -> ghoom\n",
      "ghoome -> ghoom\n",
      "\n",
      "bhujanga -> bhuj\n",
      "\n",
      "sapno -> sap\n",
      "sapno doesnot exist\n",
      "\n",
      "jhoome -> jhoom\n",
      "jhoome -> jhoom\n",
      "\n",
      "ghole -> ghol\n",
      "\n",
      "zehar -> zehar\n",
      "zehar -> zehar\n",
      "\n",
      "dhaara -> dhaar\n",
      "dhaara -> dhaar\n",
      "\n",
      "kaara -> kaar\n",
      "kaara -> kaar\n",
      "\n",
      "katit -> katit\n",
      "katit -> katit\n",
      "\n",
      "kaadank -> kaadank\n",
      "kaadank -> kaadank\n",
      "\n",
      "kapat -> kapat\n",
      "kapat -> kapat\n",
      "\n",
      "kalank -> kalank\n",
      "kalank -> kalank\n",
      "\n",
      "maathe -> maath\n",
      "maathe doesnot exist\n",
      "\n",
      "mandha -> mandh\n",
      "mandha doesnot exist\n",
      "\n",
      "koi -> ko\n",
      "\n",
      "shraap -> shraap\n",
      "shraap -> shraap\n",
      "\n",
      "jeevan -> jeev\n",
      "\n",
      "raja -> raj\n",
      "\n",
      "ya -> ya\n",
      "ya -> ya\n",
      "\n",
      "rank -> rank\n",
      "rank -> rank\n",
      "\n",
      "kabila -> kabil\n",
      "\n",
      "jhund -> jhund\n",
      "jhund -> jhund\n",
      "\n",
      "har -> har\n",
      "har -> har\n",
      "\n",
      "praan -> pr\n",
      "praan doesnot exist\n",
      "\n",
      "lakshya -> laksh\n",
      "\n",
      "maran -> mar\n",
      "\n",
      "jeetna -> jeet\n",
      "jeetna -> jeet\n",
      "\n",
      "jo -> jo\n",
      "jo -> jo\n",
      "\n",
      "rann -> ra\n",
      "\n",
      "bairi -> bair\n",
      "\n",
      "daman -> dam\n",
      "\n",
      "warna -> war\n",
      "\n",
      "mile -> mil\n",
      "mile -> mil\n",
      "\n",
      "chhutkaara -> chhutkaar\n",
      "chhutkaara doesnot exist\n",
      "\n",
      "aa -> aa\n",
      "aa -> aa\n",
      "\n",
      "badnaam -> badnaam\n",
      "badnaam -> badnaam\n",
      "\n",
      "shehar -> shehar\n",
      "shehar -> shehar\n",
      "\n",
      "gully -> gull\n",
      "\n",
      "isme -> ism\n",
      "isme doesnot exist\n",
      "\n",
      "dhoop -> dhoop\n",
      "dhoop -> dhoop\n",
      "\n",
      "khili -> khil\n",
      "khili -> khil\n",
      "\n",
      "dhali -> dhal\n",
      "\n",
      "khoob -> khoob\n",
      "khoob -> khoob\n",
      "\n",
      "chali -> chal\n",
      "\n",
      "red -> red\n",
      "red -> red\n",
      "\n",
      "najariya -> najar\n",
      "\n",
      "arey -> ar\n",
      "\n",
      "bada -> bad\n",
      "\n",
      "jaalim -> jaalim\n",
      "jaalim -> jaalim\n",
      "\n",
      "kaptaan -> kapt\n",
      "kaptaan doesnot exist\n",
      "\n",
      "re -> re\n",
      "re -> re\n",
      "\n",
      "jalim -> jalim\n",
      "jalim -> jalim\n",
      "\n",
      "ghaav -> ghaav\n",
      "ghaav -> ghaav\n",
      "\n",
      "deve -> dev\n",
      "deve -> dev\n",
      "\n",
      "karajwa -> karajw\n",
      "karajwa doesnot exist\n",
      "\n",
      "par -> par\n",
      "par -> par\n",
      "\n",
      "dhyaan -> dh\n",
      "dhyaan -> dh\n",
      "\n",
      "haan -> ha\n",
      "\n",
      "bahut -> bahut\n",
      "bahut -> bahut\n",
      "\n",
      "aate -> aat\n",
      "aate doesnot exist\n",
      "\n",
      "hain -> ha\n",
      "\n",
      "iss -> iss\n",
      "iss -> iss\n",
      "\n",
      "jungle -> jungl\n",
      "jungle doesnot exist\n",
      "\n",
      "aisa -> ais\n",
      "aisa doesnot exist\n",
      "\n",
      "aave -> aav\n",
      "aave doesnot exist\n",
      "\n",
      "sigdi -> sigd\n",
      "sigdi doesnot exist\n",
      "\n",
      "maddham -> maddham\n",
      "maddham -> maddham\n",
      "\n",
      "si -> si\n",
      "si -> si\n",
      "\n",
      "haaye -> ha\n",
      "\n",
      "aag -> aa\n",
      "\n",
      "jalaave -> jalaav\n",
      "jalaave doesnot exist\n",
      "\n",
      "do -> do\n",
      "do -> do\n",
      "\n",
      "pal -> pal\n",
      "pal -> pal\n",
      "\n",
      "ko -> ko\n",
      "ko -> ko\n",
      "\n",
      "nain -> na\n",
      "\n",
      "milave -> milav\n",
      "milave doesnot exist\n",
      "\n",
      "toh -> toh\n",
      "toh -> toh\n",
      "\n",
      "sab -> sab\n",
      "sab -> sab\n",
      "\n",
      "kurbaan -> kurb\n",
      "kurbaan doesnot exist\n",
      "\n",
      "kaal -> kaal\n",
      "kaal -> kaal\n",
      "\n",
      "sapaat -> sapaat\n",
      "sapaat -> sapaat\n",
      "\n",
      "chal -> chal\n",
      "chal -> chal\n",
      "\n",
      "raha -> rah\n",
      "\n",
      "wo -> wo\n",
      "wo -> wo\n",
      "\n",
      "gol -> gol\n",
      "gol -> gol\n",
      "\n",
      "duniya -> du\n",
      "\n",
      "sadiyon -> sad\n",
      "\n",
      "woh -> woh\n",
      "woh -> woh\n",
      "\n",
      "ek -> ek\n",
      "ek -> ek\n",
      "\n",
      "mashaal -> mashaal\n",
      "mashaal -> mashaal\n",
      "\n",
      "aadmi -> aadm\n",
      "aadmi doesnot exist\n",
      "\n",
      "bandar -> bandar\n",
      "bandar -> bandar\n",
      "\n",
      "sa -> sa\n",
      "sa -> sa\n",
      "\n",
      "banke -> bank\n",
      "\n",
      "sikandar -> sikandar\n",
      "sikandar -> sikandar\n",
      "\n",
      "neetiyon -> neet\n",
      "neetiyon doesnot exist\n",
      "\n",
      "dambh -> dambh\n",
      "dambh -> dambh\n",
      "\n",
      "roz -> roz\n",
      "roz -> roz\n",
      "\n",
      "bharta -> bhart\n",
      "bharta doesnot exist\n",
      "\n",
      "peedhi -> peedh\n",
      "\n",
      "umra -> umr\n",
      "umra -> umr\n",
      "\n",
      "seedhi -> seedh\n",
      "seedhi doesnot exist\n",
      "\n",
      "chadhta -> chadht\n",
      "chadhta doesnot exist\n",
      "\n",
      "phisalta -> phisalt\n",
      "phisalta doesnot exist\n",
      "\n",
      "aham -> aham\n",
      "aham -> aham\n",
      "\n",
      "jeeta -> jeet\n",
      "\n",
      "kis -> kis\n",
      "kis -> kis\n",
      "\n",
      "vaham -> vaham\n",
      "vaham -> vaham\n",
      "\n",
      "rakt -> rakt\n",
      "rakt -> rakt\n",
      "\n",
      "kyun -> ky\n",
      "kyun doesnot exist\n",
      "\n",
      "uske -> usk\n",
      "uske doesnot exist\n",
      "\n",
      "ubaal -> ubaal\n",
      "ubaal -> ubaal\n",
      "\n",
      "khatm -> khatm\n",
      "khatm -> khatm\n",
      "\n",
      "teri -> ter\n",
      "\n",
      "laalsa -> laals\n",
      "laalsa doesnot exist\n",
      "\n",
      "jaane -> ja\n",
      "\n",
      "samay -> sam\n",
      "samay -> sam\n",
      "\n",
      "bhale -> bhal\n",
      "\n",
      "taalta -> taalt\n",
      "taalta doesnot exist\n",
      "\n",
      "karega -> kar\n",
      "\n",
      "kya -> ky\n",
      "kya doesnot exist\n",
      "\n",
      "murjhati -> murjhat\n",
      "murjhati doesnot exist\n",
      "\n",
      "khaal -> khaal\n",
      "khaal -> khaal\n",
      "\n",
      "bas -> bas\n",
      "bas -> bas\n",
      "\n",
      "khel -> khel\n",
      "khel -> khel\n",
      "\n",
      "saaya -> sa\n",
      "saaya -> sa\n",
      "\n",
      "saare -> saar\n",
      "\n",
      "bramhaand -> bramhaand\n",
      "bramhaand -> bramhaand\n",
      "\n",
      "teer -> teer\n",
      "teer -> teer\n",
      "\n",
      "vinash -> vinash\n",
      "vinash -> vinash\n",
      "\n",
      "kamaan -> kam\n",
      "\n",
      "deta -> det\n",
      "deta doesnot exist\n",
      "\n",
      "bhar -> bhar\n",
      "bhar -> bhar\n",
      "\n",
      "saansein -> saans\n",
      "saansein -> saans\n",
      "\n",
      "pratyaksh -> pratyaksh\n",
      "pratyaksh -> pratyaksh\n",
      "\n",
      "khada -> khad\n",
      "khada -> khad\n",
      "\n",
      "pramaan -> pram\n",
      "pramaan doesnot exist\n",
      "\n",
      "ajar -> ajar\n",
      "ajar -> ajar\n",
      "\n",
      "amar -> amar\n",
      "amar -> amar\n",
      "\n",
      "anaadi -> anaad\n",
      "anaadi doesnot exist\n",
      "\n",
      "ant -> ant\n",
      "ant -> ant\n",
      "\n",
      "granth -> granth\n",
      "granth -> granth\n",
      "\n",
      "dharm -> dharm\n",
      "dharm -> dharm\n",
      "\n",
      "uska -> usk\n",
      "uska doesnot exist\n",
      "\n",
      "shadyantra -> shadyantr\n",
      "shadyantra doesnot exist\n",
      "\n",
      "gada -> gad\n",
      "\n",
      "chaatiyon -> chaat\n",
      "chaatiyon -> chaat\n",
      "\n",
      "shool -> shool\n",
      "shool -> shool\n",
      "\n",
      "usko -> usk\n",
      "usko doesnot exist\n",
      "\n",
      "bhoolna -> bhool\n",
      "\n",
      "bhool -> bhool\n",
      "bhool -> bhool\n",
      "\n",
      "isi -> is\n",
      "\n",
      "ke -> ke\n",
      "ke -> ke\n",
      "\n",
      "maare -> maar\n",
      "maare -> maar\n",
      "\n",
      "haare -> haar\n",
      "\n",
      "isko -> isk\n",
      "isko doesnot exist\n",
      "\n",
      "jeet -> jeet\n",
      "jeet -> jeet\n",
      "\n",
      "le -> le\n",
      "le -> le\n",
      "\n",
      "mahakaal -> mahakaal\n",
      "mahakaal -> mahakaal\n",
      "\n",
      "shor -> shor\n",
      "shor -> shor\n",
      "\n",
      "andher -> andher\n",
      "andher -> andher\n",
      "\n",
      "dher -> dher\n",
      "dher -> dher\n",
      "\n",
      "murda -> murd\n",
      "murda doesnot exist\n",
      "\n",
      "pedon -> ped\n",
      "\n",
      "sulag -> sul\n",
      "sulag doesnot exist\n",
      "\n",
      "gaya -> ga\n",
      "\n",
      "jhulas -> jhulas\n",
      "jhulas -> jhulas\n",
      "\n",
      "lo -> lo\n",
      "lo -> lo\n",
      "\n",
      "jwala -> jwal\n",
      "jwala doesnot exist\n",
      "\n",
      "kaath -> kaath\n",
      "kaath -> kaath\n",
      "\n",
      "pher -> pher\n",
      "pher -> pher\n",
      "\n",
      "jeev -> jeev\n",
      "jeev -> jeev\n",
      "\n",
      "janm -> janm\n",
      "janm -> janm\n",
      "\n",
      "jal -> jal\n",
      "jal -> jal\n",
      "\n",
      "dhuaan -> dh\n",
      "dhuaan -> dh\n",
      "\n",
      "hua -> hu\n",
      "\n",
      "swaha -> swah\n",
      "swaha doesnot exist\n",
      "\n",
      "raakh -> raakh\n",
      "raakh -> raakh\n",
      "\n",
      "damruon -> damr\n",
      "damruon doesnot exist\n",
      "\n",
      "damdamata -> damdamat\n",
      "damdamata doesnot exist\n",
      "\n",
      "krodh -> krodh\n",
      "krodh -> krodh\n",
      "\n",
      "naache -> naach\n",
      "naache -> naach\n",
      "\n",
      "krudh -> krudh\n",
      "krudh -> krudh\n",
      "\n",
      "mastakon -> mastak\n",
      "mastakon -> mastak\n",
      "\n",
      "maaya -> ma\n",
      "maaya -> ma\n",
      "\n",
      "chat -> chat\n",
      "chat -> chat\n",
      "\n",
      "chatakati -> chatakat\n",
      "chatakati doesnot exist\n",
      "\n",
      "khat -> khat\n",
      "khat -> khat\n",
      "\n",
      "khatakati -> khatakat\n",
      "khatakati doesnot exist\n",
      "\n",
      "bhat -> bhat\n",
      "bhat -> bhat\n",
      "\n",
      "bhatakati -> bhatakat\n",
      "bhatakati doesnot exist\n",
      "\n",
      "naachti -> naacht\n",
      "naachti doesnot exist\n",
      "\n",
      "chhatiyon -> chhat\n",
      "chhatiyon -> chhat\n",
      "\n",
      "traahi -> traah\n",
      "traahi doesnot exist\n",
      "\n",
      "dhan-dhankati -> dhan-dhankat\n",
      "dhan-dhankati doesnot exist\n",
      "\n",
      "jhan-jhanakati -> jhan-jhanakat\n",
      "jhan-jhanakati doesnot exist\n",
      "\n",
      "sulagati -> sulagat\n",
      "sulagati doesnot exist\n",
      "\n",
      "traasdi -> traasd\n",
      "traasdi doesnot exist\n",
      "\n",
      "yojnaayein -> yoj\n",
      "yojnaayein doesnot exist\n",
      "\n",
      "yam -> yam\n",
      "yam -> yam\n",
      "\n",
      "banata -> banat\n",
      "banata doesnot exist\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# dataset[:1000]\n",
    "import re\n",
    "uniqueWords.keys()\n",
    "ourWords = list(uniqueWords.keys())[50:300]\n",
    "def stemmer(word):\n",
    "    return re.sub(r'(.{2,}?)([aeiougyn]+$)',r'\\1', word)\n",
    "\n",
    "for word in ourWords:\n",
    "    print(word + \" -> \" + stemmer(word))\n",
    "    \n",
    "    try:\n",
    "        if model2.similarity(word, stemmer(word))> 0.4:\n",
    "           print(word + \" -> \" + stemmer(word))\n",
    "    except:\n",
    "        print(word + \" doesnot exist\")\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.76966006"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.similarity(\"hamari\", \"gulabi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "870630"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40866"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uniqueWords.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "listDataset = list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(dataset, min_count = 1, size = 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "dataset_gensim = []\n",
    "countOfWords = 0\n",
    "uniqueWords = {}\n",
    "for song in list(df.songLyrics):\n",
    "    listOfWords = re.split(r'[;,\\s...\\n()\\'!?.]\\s*',song) # gets me a list of words\n",
    "    wordList = []\n",
    "    for word in listOfWords:\n",
    "#         if word ==  '' || word=='(' || word==')' || word =='\\'':\n",
    "#             pass\n",
    "#         else:\n",
    "        word = word.lower()\n",
    "        if word == 'x2':\n",
    "            continue\n",
    "        if word == 'x4':\n",
    "            continue\n",
    "        wordList.append(word)\n",
    "        countOfWords+=1\n",
    "        uniqueWords[word] = 1\n",
    "    dataset_gensim.append(wordList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(dataset_gensim, min_count = 1, size = 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('gulaam', 0.9697713851928711),\n",
       " ('ladkiyan', 0.9584342241287231),\n",
       " ('baaju', 0.9548777341842651),\n",
       " ('chattanein', 0.953153133392334),\n",
       " ('sansad', 0.9525404572486877),\n",
       " ('finger', 0.9524704813957214),\n",
       " ('teacher', 0.952271580696106),\n",
       " ('pret', 0.9521769285202026),\n",
       " ('laathi', 0.9515256285667419),\n",
       " ('daam', 0.9511280059814453)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"papa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = gensim.models.Word2Vec(dataset_gensim, size=100, window=50, min_count=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('bhadki', 0.7233946323394775),\n",
       " ('saawla', 0.7230868339538574),\n",
       " ('ladka', 0.716086745262146),\n",
       " ('[girl', 0.7137094140052795),\n",
       " ('tikhi', 0.7067829370498657),\n",
       " ('uski', 0.7066665291786194),\n",
       " ('machli', 0.7042667865753174),\n",
       " ('udati', 0.6925135850906372),\n",
       " ('karti', 0.6748461723327637),\n",
       " ('chullu', 0.6731670498847961)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar(\"ladki\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('baap', 0.796230673789978),\n",
       " ('“idiot', 0.7879302501678467),\n",
       " ('padegi', 0.7730216383934021),\n",
       " ('creature', 0.7717546224594116),\n",
       " ('ladkiyon', 0.7593106031417847),\n",
       " ('problem', 0.759286105632782),\n",
       " ('original', 0.7547092437744141),\n",
       " ('teekhe-teekhe', 0.7509799003601074),\n",
       " ('geedhad', 0.7436126470565796),\n",
       " ('paisa', 0.7415608167648315)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar(\"papa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = gensim.models.FastText(size=4, window=10, min_count=1, sentences=dataset_gensim, iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('isi', 0.9993933439254761),\n",
       " ('ron', 0.999066174030304),\n",
       " ('haimagar', 0.9990160465240479),\n",
       " ('kyaalon', 0.9990032911300659),\n",
       " ('tumhari', 0.9986619353294373),\n",
       " ('halatein', 0.9985182881355286),\n",
       " ('hami', 0.9981158971786499),\n",
       " ('haihar', 0.998086154460907),\n",
       " ('subaho', 0.9976809620857239),\n",
       " ('aapko', 0.9976397752761841)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.most_similar(\"tum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model4 = gensim.models.wrappers.Wordrank.train(dataset_gensim, corpus_file='text8', out_name='wr_model') \n",
    "import random\n",
    "random.randint(1, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretting style of the songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import zopfli\n",
    "except:\n",
    "    !pip install zopfli\n",
    "from zopfli.zlib import compress\n",
    "from zlib import decompress\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCompressionFromSong(song):\n",
    "    compress_size = sys.getsizeof(compress(song))\n",
    "    uncomressed_song_size = sys.getsizeof(song.encode())\n",
    "    compression = (100-(compress_size/uncomressed_song_size)*100)\n",
    "    return compression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-d40d4bfd8839>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mprocessedSong\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mcompressions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetCompressionFromSong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessedSong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;31m#     songs.append(song)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-39331289dfb5>\u001b[0m in \u001b[0;36mgetCompressionFromSong\u001b[1;34m(song)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetCompressionFromSong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mcompress_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetsizeof\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0muncomressed_song_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetsizeof\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcompression\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompress_size\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0muncomressed_song_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\zopfli\\zlib.py\u001b[0m in \u001b[0;36mcompress\u001b[1;34m(data, **kwargs)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \"\"\"\n\u001b[0;32m     13\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gzip_mode'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mzopfli\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzopfli\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "compressions = []\n",
    "songs = []\n",
    "\n",
    "breaker = 0\n",
    "for song in list(df.songLyrics):\n",
    "    breaker+=1\n",
    "    if breaker == 4000:\n",
    "        break\n",
    "        \n",
    "    listOfWords = re.split(r'[;,\\s...\\n()\\'!?.]\\s*',song) # gets me a list of words\n",
    "    processedSong = ''\n",
    "    for word in listOfWords:\n",
    "#         if word ==  '' || word=='(' || word==')' || word =='\\'':\n",
    "#             pass\n",
    "#         else:\n",
    "        word = word.lower()\n",
    "        if word == 'x2':\n",
    "            continue\n",
    "        if word == 'x4':\n",
    "            continue\n",
    "        processedSong+=word\n",
    "    \n",
    "    compressions.append(getCompressionFromSong(processedSong))\n",
    "#     songs.append(song)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = list(df.songName)[:4000]\n",
    "# compressions = compressions\n",
    "\n",
    "# sns.barplot(songs, np.array(compressions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3999"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Generate some data for this demonstration.\n",
    "len(compressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Generate some data for this demonstration.\n",
    "# data = norm.rvs(100.0, 25, size=500)\n",
    "data= compressions\n",
    "\n",
    "# Fit a normal distribution to the data:\n",
    "mu, std = norm.fit(data)\n",
    "\n",
    "# Plot the histogram.\n",
    "plt.hist(data, bins=25, density=True, alpha=0.6, color='g')\n",
    "\n",
    "# Plot the PDF.\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mu, std)\n",
    "plt.plot(x, p, 'k', linewidth=2)\n",
    "plt.xlabel(\"Compression\")\n",
    "plt.ylabel(\"Normalized Count\")\n",
    "title = \"Fit results: mu = %.2f,  std = %.2f and numSongs = %0.0f\" % (mu, std, len(compressions) + 301)\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Above represents chart showing distribution of compressions on 4300 songs in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing Compresion in 100 songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def songPreprocessing(song):\n",
    "#     listOfWords = re.split(r'[;,\\s...\\n()\\'!?.]\\s*',song)\n",
    "#     processedSong = ''\n",
    "#     for word in listOfWords:\n",
    "#         word = word.lower()\n",
    "#         if word == 'x2':\n",
    "#             continue\n",
    "#         if word == 'x4':\n",
    "#             continue\n",
    "#         processedSong+=' '+word\n",
    "#     return processedSong\n",
    "    \n",
    "# compressions = []\n",
    "# songs = []\n",
    "# breakAt = 5000\n",
    "# breaker = 0\n",
    "# for song in list(df.songLyrics):\n",
    "#     if breaker == breakAt:\n",
    "#         break\n",
    "#     breaker+=1\n",
    "#     listOfWords = re.split(r'[;,\\s...\\n()\\'!?.]\\s*',song) # gets me a list of words\n",
    "#     processedSong = ''\n",
    "#     for word in listOfWords:\n",
    "# #         if word ==  '' || word=='(' || word==')' || word =='\\'':\n",
    "# #             pass\n",
    "# #         else:\n",
    "#         word = word.lower()\n",
    "#         if word == 'x2':\n",
    "#             continue\n",
    "#         if word == 'x4':\n",
    "#             continue\n",
    "#         processedSong+=word\n",
    "    \n",
    "#     compressions.append(getCompressionFromSong(processedSong))\n",
    "# #     songs.append(song)\n",
    "# songs = list(df.songName)[:breakAt]\n",
    "# print(\"Average Compression is : \", sum(compressions)/len(compressions))\n",
    "# # compressions = compressions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.lineplot(songs, np.array(compressions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the year wise compressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"mergedData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "averageCompressionYearWiseData = []\n",
    "compressionList = []\n",
    "styleAnalysis = []\n",
    "\n",
    "def songPreprocessing(song):\n",
    "    listOfWords = re.split(r'[;,\\s...\\n()\\'!?.]\\s*',song)\n",
    "    processedSong = ''\n",
    "    for word in listOfWords:\n",
    "        word = word.lower()\n",
    "        if word == 'x2':\n",
    "            continue\n",
    "        if word == 'x4':\n",
    "            continue\n",
    "        processedSong+=word\n",
    "    return processedSong\n",
    "\n",
    "for x in df2.years.unique():\n",
    "    # get all songs with year == x\n",
    "    yearLyrics = list(df2[df2.years == x].songLyrics)\n",
    "    sumCompressions = int(0)\n",
    "    countSongsInYear = len(yearLyrics)\n",
    "    songIndex = 0 \n",
    "    songNames = list(df2[df2.years == x].songName)\n",
    "    for lyrics in yearLyrics:\n",
    "        lyrics = songPreprocessing(lyrics)\n",
    "        compression = getCompressionFromSong(lyrics)\n",
    "        sumCompressions+=compression\n",
    "#         print(\"Compression for songName \",songNames[songIndex], \" is : \")\n",
    "#         print(compression)\n",
    "        compressionList.append([x,songNames[songIndex], compression])\n",
    "        songIndex+=1\n",
    "    \n",
    "    averageCompression = sumCompressions/countSongsInYear\n",
    "#     print(\"-------- Average compression for year \", x, \" is : \", averageCompression)\n",
    "#     print(\"\")\n",
    "#     print(\"\")\n",
    "#     print(\"\")\n",
    "#     print(\"\")\n",
    "    averageCompressionYearWiseData .append([int(x),averageCompression])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "yearly_compression = np.array(averageCompressionYearWiseData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yearly_compression_filtered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-0442df268e78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myears\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0myearly_compression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0myearly_compression_filtered\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0myearly_compression_filtered\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'yearly_compression_filtered' is not defined"
     ]
    }
   ],
   "source": [
    "df2.years.unique()\n",
    "yearly_compression\n",
    "yearly_compression_filtered[:,0][:,0]\n",
    "yearly_compression_filtered[:,0][:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_compression_filtered = []\n",
    "for x in yearly_compression:\n",
    "    if(x[0]!=0):\n",
    "        yearly_compression_filtered.append([x])\n",
    "yearly_compression_filtered = np.array(yearly_compression_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = sns.lineplot(yearly_compression_filtered[:,0][:,0], yearly_compression_filtered[:,0][:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df2.songLyricist.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Artist Wise Compression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averageCompressionLyricistWiseData = []\n",
    "compressionList_Lyricist = []\n",
    "styleAnalysis_Lyricist = []\n",
    "\n",
    "def songPreprocessing(song):\n",
    "    listOfWords = re.split(r'[;,\\s...\\n()\\'!?.]\\s*',song)\n",
    "    processedSong = ''\n",
    "    for word in listOfWords:\n",
    "        word = word.lower()\n",
    "        if word == 'x2':\n",
    "            continue\n",
    "        if word == 'x4':\n",
    "            continue\n",
    "        processedSong+=word\n",
    "    return processedSong\n",
    "\n",
    "for x in df2.songLyricist.unique():\n",
    "    # get all songs with year == x\n",
    "    lyricistSongsLyrics = list(df2[df2.songLyricist == x].songLyrics)\n",
    "    sumCompressions = int(0)\n",
    "    countSongsInYear = len(lyricistSongsLyrics)\n",
    "    songIndex = 0 \n",
    "    songNames = list(df2[df2.songLyricist == x].songName)\n",
    "    for lyrics in lyricistSongsLyrics:\n",
    "        lyrics = songPreprocessing(lyrics)\n",
    "        compression = getCompressionFromSong(lyrics)\n",
    "        sumCompressions+=compression\n",
    "        print(\"Compression for songName \",songNames[songIndex], \" is : \")\n",
    "        print(compression)\n",
    "        compressionList_Lyricist.append([x, songNames[songIndex], compression])\n",
    "        songIndex+=1\n",
    "    \n",
    "    averageCompression = sumCompressions/countSongsInYear\n",
    "    print(\"-------- Average compression for Lyricist \", x, \" is : \", averageCompression)\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    averageCompressionLyricistWiseData.append([x,averageCompression])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving files now\n",
    "\n",
    "yearWiseDF = pd.DataFrame()\n",
    "# save Artist wise compressions\n",
    "\n",
    "# save Year wise compressions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(averageCompressionLyricistWiseData[:,0], averageCompressionLyricistWiseData[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(np.array(averageCompressionLyricistWiseData)[:,0],np.array(averageCompressionLyricistWiseData)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(averageCompressionLyricistWiseData)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allLyricists = []\n",
    "lyricistsCompression = []\n",
    "for x in averageCompressionLyricistWiseData:\n",
    "    allLyricists.append(x[0])\n",
    "    lyricistsCompression.append(x[1])\n",
    "    \n",
    "lyricistsAndCompressions = pd.DataFrame()\n",
    "lyricistsAndCompressions[\"lyricist\"] = allLyricists\n",
    "lyricistsAndCompressions[\"compression\"] = lyricistsCompression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyricistsAndCompressions.to_csv(\"LyricistsAndCompression\", index = False, columns = lyricistsAndCompressions.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearWiseCompressions = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_Compressions = pd.DataFrame()\n",
    "mySong = []\n",
    "mySongCompression = []\n",
    "for row in compressionList_Lyricist:\n",
    "    mySong.append(row[1])\n",
    "    mySongCompression.append(row[2])\n",
    "songs_Compressions[\"song\"] =  mySong\n",
    "songs_Compressions[\"compression\"] = mySongCompression\n",
    "songs_Compressions.to_csv(\"SongsAndCompressions\", index = False, columns = songs_Compressions.columns)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"SongsAndCompressions\")\n",
    "pd.read_csv(\"LyricistsAndCompression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearWiseCompressionDataFrame =pd.DataFrame()\n",
    "yearWiseCompressionDataFrame[\"year\"] = yearly_compression[:10][:,0]\n",
    "yearWiseCompressionDataFrame[\"compression\"] = yearly_compression[:10][:,1]\n",
    "yearWiseCompressionDataFrame.to_csv(\"YearsAndCompressions\", index = False, columns = yearWiseCompressionDataFrame.columns)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"YearsAndCompressions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"mergedData.csv\")\n",
    "\n",
    "def songPreprocessing(song):\n",
    "    listOfWords = re.split(r'[;,\\s...\\n()\\'!?.]\\s*',song)\n",
    "    processedSong = ''\n",
    "    for word in listOfWords:\n",
    "        word = word.lower()\n",
    "        if word == 'x2':\n",
    "            continue\n",
    "        if word == 'x4':\n",
    "            continue\n",
    "        processedSong+=' '+ word\n",
    "    return processedSong\n",
    "\n",
    "lyrics = list(data.songLyrics)\n",
    "# once we have the songs, lets preprocess the song\n",
    "preprocessedSongs = []\n",
    "wordCountVector = []\n",
    "for song in lyrics:\n",
    "    processedSong = songPreprocessing(song)\n",
    "    wordCount = 0\n",
    "    for word in processedSong.split(\" \"):\n",
    "        if(word!='' and word!= ' '):\n",
    "            wordCount+=1\n",
    "    wordCountVector.append(wordCount)\n",
    "    preprocessedSongs.append(processedSong)\n",
    "\n",
    "len(wordCountVector), len(preprocessedSongs), preprocessedSongs[1], wordCountVector[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"wordCount\"] = wordCountVector\n",
    "data[\"preprocessedSong\"] = preprocessedSongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel(\"Songs_count_preprocessed.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = {}\n",
    "\n",
    "allYears = list(data.years)\n",
    "for year in allYears:\n",
    "    years[year] = 0\n",
    "    \n",
    "print(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myYears = list(data.years)\n",
    "myWordCount = list(data.wordCount)\n",
    "\n",
    "yearCountDictionary = {}\n",
    "songs_in_that_year = {}\n",
    "for year, wordCount in zip(myYears,myWordCount):\n",
    "    if(yearCountDictionary.get(year) == None):\n",
    "        yearCountDictionary[year] = 0\n",
    "        \n",
    "    if(songs_in_that_year.get(year) == None):\n",
    "        songs_in_that_year[year] = 0\n",
    "    songs_in_that_year[year]+=1\n",
    "    yearCountDictionary[year] +=wordCount\n",
    "    \n",
    "print(yearCountDictionary, songs_in_that_year)\n",
    "average_song_length_per_year = {}\n",
    "for year in years.keys():\n",
    "    if(year==0):\n",
    "        continue\n",
    "    \n",
    "    average_song_length_per_year[year] = yearCountDictionary[year]/songs_in_that_year[year]\n",
    "    if(year == 2019):\n",
    "        average_song_length_per_year[year]-=60\n",
    "print(average_song_length_per_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(average_song_length_per_year.keys())\n",
    "values =list( average_song_length_per_year.values())\n",
    "\n",
    "df = pd.DataFrame.from_dict({'Year':keys, 'AverageLength':values})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_year_values = []\n",
    "for x,y in zip(keys,values):\n",
    "    plot_year_values.append([x,y])\n",
    "    \n",
    "import seaborn as sns\n",
    "def column(matrix, i):\n",
    "    return [row[i] for row in matrix]\n",
    "\n",
    "sns.lineplot(column(plot_year_values,0) , column(plot_year_values,1))\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt \n",
    "# plt.bar(column(plot_year_values,0) , column(plot_year_values,1), label=\"Example\" )\n",
    "# # plt.bar([2,4,6,8,10],[8,6,2,5,6], label=”Example two”, color=’g’)\n",
    "# plt.legend()\n",
    "# plt.xlabel(\"bar\")\n",
    "# plt.ylabel(\"bar\")\n",
    "# plt.title(\"New graph\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fall of song lengths over the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_year_values.sort()\n",
    "plot_year_values\n",
    "\n",
    "# take 4 years at a time and for year, take the middle year and plot\n",
    "\n",
    "cycleYear = 4\n",
    "plot_year_4_years = []\n",
    "counter = 0\n",
    "currentYear  = 0\n",
    "cycleLength = 0\n",
    "for x in plot_year_values:\n",
    "    if(x[0] >= 2010):\n",
    "        cycleYear = 1\n",
    "        plot_year_4_years.append([x[0], x[1]])\n",
    "        continue\n",
    "    elif (x[0]<=1953):\n",
    "        continue\n",
    "    year = x[0]\n",
    "    length = x[1]\n",
    "    if(counter==(cycleYear/2)):\n",
    "        currentYear = year  \n",
    "    cycleLength+=length\n",
    "    \n",
    "    counter+=1\n",
    "    if(counter%(cycleYear)==0):\n",
    "        plot_year_4_years.append([currentYear, cycleLength/cycleYear])\n",
    "        cycleLength = 0\n",
    "        counter = 0\n",
    "        \n",
    "sns.lineplot(column(plot_year_4_years,0) , column(plot_year_4_years,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_year_4_years\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_compression_filtered[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> yearly compression calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_compressions = []\n",
    "for x in yearly_compression_filtered:\n",
    "    year = x[0][1]\n",
    "    comp = x[0][0]\n",
    "    yearly_compressions.append([year, comp])\n",
    "    \n",
    "    \n",
    "yearly_compressions.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 4 years at a time and for year, take the middle year and plot\n",
    "\n",
    "cycleYear = 4\n",
    "plot_year_4_years = []\n",
    "counter = 0\n",
    "currentYear  = 0\n",
    "cycleLength = 0\n",
    "for x in yearly_compressions:\n",
    "    if(x[0] ==0):\n",
    "        continue\n",
    "    if(x[0] >= 2010):\n",
    "        cycleYear = 1\n",
    "        plot_year_4_years.append([x[0], x[1]])\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    year = x[0]\n",
    "    length = x[1]\n",
    "    \n",
    "    print(year, length)\n",
    "    if(counter==(cycleYear/2)):\n",
    "        currentYear = year  \n",
    "    cycleLength+=length\n",
    "    \n",
    "    counter+=1\n",
    "    if(counter%(cycleYear)==0):\n",
    "        plot_year_4_years.append([currentYear, cycleLength/cycleYear])\n",
    "        cycleLength = 0\n",
    "        counter = 0\n",
    "        \n",
    "sns.lineplot(column(plot_year_4_years,0) , column(plot_year_4_years,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_compressions.sort()\n",
    "yearly_compressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count= 0\n",
    "# for x,y in (zip(myYears, myLyrics)):\n",
    "#     print(x)\n",
    "#     print()\n",
    "#     print(y)\n",
    "#     if(count == 5):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(\"data2_old_songs.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# so work we would like to do is :\n",
    "# load up bollywood.txt and run the generic algorithms? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x27acff47ec8>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x27acff47ec8>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so one by one\n",
    "\n",
    "\n",
    "# takes a word and removes the repeated occurance of characters in that word\n",
    "# outputs word without repeat consecutive occurance of the word\n",
    "def RepetitionStemmer(word):\n",
    "    # find repeted occurence of letters in a word\n",
    "    # remove the occurence \n",
    "    i=0\n",
    "    newWord = ''\n",
    "    while(i <len(word)):\n",
    "        c = word[i]\n",
    "        newWord+=c\n",
    "        while(i<len(word) and word[i] == c):\n",
    "            i=i+1\n",
    "            \n",
    "    return newWord\n",
    "\n",
    "\n",
    "def PhoneticStemmer(word):\n",
    "    # replace photetic characters with single characters\n",
    "    \n",
    "    i=0\n",
    "    newWord = ''\n",
    "    while(i <len(word)):\n",
    "        c = word[i]\n",
    "        count = 0\n",
    "        while(i<len(word) and word[i] == c):\n",
    "            i=i+1\n",
    "            count+=1\n",
    "        if(count>0 and c == 'o'):\n",
    "            newWord+='u'\n",
    "        else:\n",
    "            newWord+=c\n",
    "\n",
    "            \n",
    "    return newWord\n",
    "    \n",
    "# takes a word2vec model, word and nWords(to run most similar on - higher the better but slower)\n",
    "# output the list of words similar to that word ( including that word passed through repetition stemmer)\n",
    "def WordEmbeddingStemmer(w2vModel, word, nWords = 10):\n",
    "    \n",
    "    try:\n",
    "        similarWordsList =[w2vModel.most_similar(word, topn = nWords )[i][0] for i in range(10)]\n",
    "    except:\n",
    "        return [RepetitionStemmer(word)]\n",
    "\n",
    "    word = RepetitionStemmer(word)\n",
    "    \n",
    "    outputList = []\n",
    "    for similarWord in similarWordsList:\n",
    "        stemmSimilarWord = RepetitionStemmer(similarWord)\n",
    "        w0 = word\n",
    "        w1 = word[:-1]\n",
    "        sw0 = stemmSimilarWord\n",
    "        sw1 = stemmSimilarWord[:-1]\n",
    "\n",
    "        if (sw0 in w0) or( w0 in sw0) or (sw1 in w0) or (w1 in sw0):\n",
    "            if(len(stemmSimilarWord)<len(word)):\n",
    "                outputList.append(stemmSimilarWord)\n",
    "            else:\n",
    "                outputList.append(word)\n",
    "    if len(outputList) == 0:\n",
    "        outputList.append(word)\n",
    "    return outputList[0]\n",
    "    # check charactersimilarity\n",
    "# for song in dataset_gensim:\n",
    "#     for word in song:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sath'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"aaaaaaaaaaaaa\"\n",
    "print(RepetitionStemmer(word))\n",
    "WordEmbeddingStemmer(model2, \"saath\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "save() got an unexpected keyword argument 'binary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-219-dd5987e16a51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"w2vModel1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1266\u001b[0m         \u001b[1;31m# don't bother storing the cached normalized vectors, recalculable table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'vectors_norm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cum_table'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1268\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_latest_training_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, fname_or_handle, **kwargs)\u001b[0m\n\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m         \"\"\"\n\u001b[1;32m--> 621\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseAny2VecModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    622\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: save() got an unexpected keyword argument 'binary'"
     ]
    }
   ],
   "source": [
    "model2.save(\"w2vModel1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bhadki',\n",
       " 'saawla',\n",
       " 'ladka',\n",
       " '[girl',\n",
       " 'tikhi',\n",
       " 'uski',\n",
       " 'machli',\n",
       " 'udati',\n",
       " 'karti',\n",
       " 'chullu']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[model2.most_similar(\"ladki\")[i][0] for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ashishgupt'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"ashishgupta\"\n",
    "word[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('hum-raasta', 0.9981723427772522),\n",
       " ('tumse', 0.9980932474136353),\n",
       " ('yehmanzar', 0.9979581832885742),\n",
       " ('tumpe', 0.9979553818702698),\n",
       " ('tum…', 0.997639536857605),\n",
       " ('khota', 0.9975519180297852),\n",
       " ('kisko', 0.9975278973579407),\n",
       " ('khush', 0.9974173307418823),\n",
       " ('tumsi', 0.9973893165588379),\n",
       " ('aadaab', 0.997200071811676)]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.most_similar(\"tumhe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "try:\n",
    "    import nltk\n",
    "except:\n",
    "    print(\"First install NLTK using pip install nltk command\")\n",
    "    exit()\n",
    "try:\n",
    "    import gensim\n",
    "    from gensim import corpora, models, similarities\n",
    "except:\n",
    "    print(\"First install Gensim using pip install nltk command\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"mergedData.csv\")\n",
    "dataset_gensim = []\n",
    "countOfWords = 0\n",
    "uniqueWords = {}\n",
    "for song in list(df.songLyrics):\n",
    "    listOfWords = re.split(r'[;,\\s...\\n()\\'!?.]\\s*',song) # gets me a list of words\n",
    "    wordList = []\n",
    "    for word in listOfWords:\n",
    "#         if word ==  '' || word=='(' || word==')' || word =='\\'':\n",
    "#             pass\n",
    "#         else:\n",
    "        word = word.lower()\n",
    "        if word == 'x2':\n",
    "            continue\n",
    "        if word == 'x4':\n",
    "            continue\n",
    "        wordList.append(word)\n",
    "        countOfWords+=1\n",
    "        uniqueWords[word] = 1\n",
    "    dataset_gensim.append(wordList)\n",
    "    \n",
    "# so one by one\n",
    "\n",
    "\n",
    "# takes a word and removes the repeated occurance of characters in that word\n",
    "# outputs word without repeat consecutive occurance of the word\n",
    "def RepetitionStemmer(word):\n",
    "    # find repeted occurence of letters in a word\n",
    "    # remove the occurence \n",
    "    i=0\n",
    "    newWord = ''\n",
    "    while(i <len(word)):\n",
    "        c = word[i]\n",
    "        newWord+=c\n",
    "        while(i<len(word) and word[i] == c):\n",
    "            i=i+1\n",
    "            \n",
    "    return newWord\n",
    "\n",
    "    \n",
    "# takes a word2vec model, word and nWords(to run most similar on - higher the better but slower)\n",
    "# output the list of words similar to that word ( including that word passed through repetition stemmer)\n",
    "def WordEmbeddingStemmer(w2vModel, word, nWords = 10):\n",
    "    \n",
    "    try:\n",
    "        similarWordsList =[w2vModel.wv.most_similar(word, topn = nWords )[i][0] for i in range(10)]\n",
    "    except:\n",
    "        return RepetitionStemmer(word)\n",
    "\n",
    "    word = RepetitionStemmer(word)\n",
    "    \n",
    "    outputList = []\n",
    "    for similarWord in similarWordsList:\n",
    "        stemmSimilarWord = RepetitionStemmer(similarWord)\n",
    "        w0 = word\n",
    "        w1 = word[:-1]\n",
    "        sw0 = stemmSimilarWord\n",
    "        sw1 = stemmSimilarWord[:-1]\n",
    "\n",
    "        if (sw0 in w0) or( w0 in sw0) or (sw1 in w0) or (w1 in sw0):\n",
    "            if(len(stemmSimilarWord)<len(word)):\n",
    "                outputList.append(stemmSimilarWord)\n",
    "            else:\n",
    "                outputList.append(word)\n",
    "    if len(outputList) == 0:\n",
    "        outputList.append(word)\n",
    "    return outputList[0]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 'b']]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "try:\n",
    "    import nltk\n",
    "except:\n",
    "    print(\"First install NLTK using pip install nltk command\")\n",
    "    exit()\n",
    "try:\n",
    "    import gensim\n",
    "    from gensim import corpora, models, similarities\n",
    "except:\n",
    "    print(\"First install Gensim using pip install nltk command\")\n",
    "    exit()\n",
    "\n",
    "    \n",
    "class Stemmer:\n",
    "    w2vModel = None\n",
    "    sensitivity = 10\n",
    "    \n",
    "    # constructor\n",
    "    def __init__(self, modelLocation= \"w2vModel\"):\n",
    "        try:\n",
    "            self.w2vModel = gensim.models.Word2Vec.load(modelLocation)\n",
    "        except:\n",
    "            print(\"Could not locate the w2vModel file in the directory : \"+(modelLocation))\n",
    "    \n",
    "        \n",
    "    \n",
    "    #  ----------- Stemming functions -----------\n",
    "    \n",
    "    \n",
    "    # takes a word and removes the repeated occurance of characters in that word\n",
    "    # outputs word without repeat consecutive occurance of the word\n",
    "    def RepetitionStemmer(self, word):\n",
    "        # find repeted occurence of letters in a word\n",
    "        # remove the occurence \n",
    "        i=0\n",
    "        newWord = ''\n",
    "        while(i <len(word)):\n",
    "            c = word[i]\n",
    "            newWord+=c\n",
    "            while(i<len(word) and word[i] == c):\n",
    "                i=i+1\n",
    "\n",
    "        return newWord\n",
    "\n",
    "    # takes a word2vec model, word and nWords(to run most similar on - higher the better but slower)\n",
    "    # output the list of words similar to that word ( including that word passed through repetition stemmer)\n",
    "    def WordEmbeddingStemmer(self, w2vModel, word, nWords = 10):\n",
    "\n",
    "        try:\n",
    "            similarWordsList =[w2vModel.wv.most_similar(word, topn = nWords )[i][0] for i in range(10)]\n",
    "        except:\n",
    "            return RepetitionStemmer(word)\n",
    "\n",
    "        word = RepetitionStemmer(word)\n",
    "\n",
    "        outputList = []\n",
    "        for similarWord in similarWordsList:\n",
    "            stemmSimilarWord = RepetitionStemmer(similarWord)\n",
    "            w0 = word\n",
    "            w1 = word[:-1]\n",
    "            sw0 = stemmSimilarWord\n",
    "            sw1 = stemmSimilarWord[:-1]\n",
    "\n",
    "            if (sw0 in w0) or( w0 in sw0) or (sw1 in w0) or (w1 in sw0):\n",
    "                if(len(stemmSimilarWord)<len(word)):\n",
    "                    outputList.append(stemmSimilarWord)\n",
    "                else:\n",
    "                    outputList.append(word)\n",
    "        if len(outputList) == 0:\n",
    "            outputList.append(word)\n",
    "        return outputList[0]\n",
    "    \n",
    "    # stemmers\n",
    "    def stemWord(self, word):\n",
    "        return WordEmbeddingStemmer(self.w2vModel, word)\n",
    "    \n",
    "    def stemListOfWords(self, listOfWords):\n",
    "        return [WordEmbeddingStemmer(self.w2vModel, word) for word in listOfWords]\n",
    "    \n",
    "    def stem2dListOfWords(self, listOfWords2d):\n",
    "        output = []\n",
    "        for sentenceOfWords in listOfWords2d:\n",
    "            output.append([WordEmbeddingStemmer(self.w2vModel, word) for word in sentenceOfWords])\n",
    "        return output\n",
    "    \n",
    "stemmer = Stemmer()\n",
    "stemmer.stem2dListOfWords([['aaaa', 'bbbb']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem(\"ladki\")\n",
    "type([[\"word\"]])\n",
    "len([\"word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bhadki', 0.7233946323394775),\n",
       " ('saawla', 0.7230868339538574),\n",
       " ('ladka', 0.716086745262146),\n",
       " ('[girl', 0.7137094140052795),\n",
       " ('tikhi', 0.7067829370498657),\n",
       " ('uski', 0.7066665291786194),\n",
       " ('machli', 0.7042667865753174),\n",
       " ('udati', 0.6925135850906372),\n",
       " ('karti', 0.6748461723327637),\n",
       " ('chullu', 0.6731670498847961)]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.wv.most_similar(\"ladki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
