{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\Scripts\\pip-script.py\", line 6, in <module>\n",
      "    from pip._internal import main\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\__init__.py\", line 19, in <module>\n",
      "    from pip._vendor.urllib3.exceptions import DependencyWarning\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\__init__.py\", line 7, in <module>\n",
      "    from .connectionpool import (\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\connectionpool.py\", line 36, in <module>\n",
      "    from .request import RequestMethods\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\request.py\", line 3, in <module>\n",
      "    from .filepost import encode_multipart_formdata\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\filepost.py\", line 10, in <module>\n",
      "    from .fields import RequestField\n",
      "ValueError: source code string cannot contain null bytes\n"
     ]
    }
   ],
   "source": [
    "# find Stopwords for Hinglish- remove min_df,max_df\n",
    "# Try compressing the songs first and then pass to word2vec\n",
    "# Trends in songs over the years\n",
    "# Find the topics on which songs are focused\n",
    "# \n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import word2vec\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import seaborn as sns\n",
    "import operator\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unnamed</th>\n",
       "      <th>songLyrics</th>\n",
       "      <th>songMovie</th>\n",
       "      <th>songSinger</th>\n",
       "      <th>songName</th>\n",
       "      <th>years</th>\n",
       "      <th>compressions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ghoomar rabb waare\\r\\nAap padharo saa\\r\\n\\r\\nA...</td>\n",
       "      <td>Padmaavat</td>\n",
       "      <td>Shreya Ghoshal</td>\n",
       "      <td>Ghoomar</td>\n",
       "      <td>2018</td>\n",
       "      <td>64.734300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Yo yo honey singh…\\r\\n\\r\\nHoye…\\r\\n\\r\\nSuno ka...</td>\n",
       "      <td>Sonu Ke Titu ki Sweety</td>\n",
       "      <td>Yo Yo Honey Singh, Simar Kaur &amp; Ishers</td>\n",
       "      <td>Dil Chori</td>\n",
       "      <td>2018</td>\n",
       "      <td>65.985998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Aaj se teri saari galiyan meri ho gayi\\r\\nAaj ...</td>\n",
       "      <td>Padman</td>\n",
       "      <td>Arijit Singh</td>\n",
       "      <td>Aaj Se Teri</td>\n",
       "      <td>2018</td>\n",
       "      <td>65.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Boy you don’t have to be the last one standing...</td>\n",
       "      <td>Sonu Ke Titu Ki Sweety</td>\n",
       "      <td>Zack Knight &amp; Jasmin Walia</td>\n",
       "      <td>Bom Diggy Diggy</td>\n",
       "      <td>2018</td>\n",
       "      <td>66.455696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Pila de.. pila de..\\r\\nPila de pila de\\r\\n\\r\\n...</td>\n",
       "      <td>Sonu Ke Titu ki Sweety</td>\n",
       "      <td>Yo Yo Honey Singh, Neha Kakkar, Navraj Hans</td>\n",
       "      <td>Chhote Chhote Peg Maar</td>\n",
       "      <td>2018</td>\n",
       "      <td>62.760417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unnamed                                         songLyrics  \\\n",
       "0        0  Ghoomar rabb waare\\r\\nAap padharo saa\\r\\n\\r\\nA...   \n",
       "1        1  Yo yo honey singh…\\r\\n\\r\\nHoye…\\r\\n\\r\\nSuno ka...   \n",
       "2        2  Aaj se teri saari galiyan meri ho gayi\\r\\nAaj ...   \n",
       "3        3  Boy you don’t have to be the last one standing...   \n",
       "4        4  Pila de.. pila de..\\r\\nPila de pila de\\r\\n\\r\\n...   \n",
       "\n",
       "                songMovie                                   songSinger  \\\n",
       "0               Padmaavat                               Shreya Ghoshal   \n",
       "1  Sonu Ke Titu ki Sweety       Yo Yo Honey Singh, Simar Kaur & Ishers   \n",
       "2                  Padman                                 Arijit Singh   \n",
       "3  Sonu Ke Titu Ki Sweety                   Zack Knight & Jasmin Walia   \n",
       "4  Sonu Ke Titu ki Sweety  Yo Yo Honey Singh, Neha Kakkar, Navraj Hans   \n",
       "\n",
       "                  songName  years  compressions  \n",
       "0                  Ghoomar   2018     64.734300  \n",
       "1                Dil Chori   2018     65.985998  \n",
       "2              Aaj Se Teri   2018     65.083333  \n",
       "3          Bom Diggy Diggy   2018     66.455696  \n",
       "4   Chhote Chhote Peg Maar   2018     62.760417  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # data=pd.read_csv(\"songs7k.csv\")\n",
    "# # new addition\n",
    "# data1 = pd.read_csv(\"songs7kFormattedWithCompressions.csv\", encoding=\"string\n",
    "#             \")\n",
    "# data = data1\n",
    "# data1.head()\n",
    "\n",
    "doc=[]\n",
    "for i in range(data.shape[0]):\n",
    "    doc.append(nltk.word_tokenize(re.sub('[^a-zA-z\\s]','',data['songLyrics'][i].lower())))\n",
    "\n",
    "\n",
    "# build vocabulary and train model\n",
    "w2v_model = gensim.models.Word2Vec(doc,size=300,window=15,min_count=2,workers=10,iter=10)\n",
    " \n",
    "simWords = w2v_model.wv.most_similar(positive=[\"ladki\"],topn=30)\n",
    "myStrr = \"\"\n",
    "for x in simWords:\n",
    "    myStrr= myStrr+\" \"+ x[0]\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(myStrr)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Year'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-3b08d1fa6443>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Year'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36msort_values\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position)\u001b[0m\n\u001b[0;32m   4991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4992\u001b[0m             \u001b[0mby\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4993\u001b[1;33m             \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4994\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4995\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1772\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1773\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1774\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1776\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Year'"
     ]
    }
   ],
   "source": [
    "data = data.sort_values(by=['Year'], ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unnamed</th>\n",
       "      <th>songLyrics</th>\n",
       "      <th>songMovie</th>\n",
       "      <th>songSinger</th>\n",
       "      <th>songName</th>\n",
       "      <th>years</th>\n",
       "      <th>compressions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ghoomar rabb waare\\r\\nAap padharo saa\\r\\n\\r\\nA...</td>\n",
       "      <td>Padmaavat</td>\n",
       "      <td>Shreya Ghoshal</td>\n",
       "      <td>Ghoomar</td>\n",
       "      <td>2018</td>\n",
       "      <td>64.734300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Yo yo honey singh…\\r\\n\\r\\nHoye…\\r\\n\\r\\nSuno ka...</td>\n",
       "      <td>Sonu Ke Titu ki Sweety</td>\n",
       "      <td>Yo Yo Honey Singh, Simar Kaur &amp; Ishers</td>\n",
       "      <td>Dil Chori</td>\n",
       "      <td>2018</td>\n",
       "      <td>65.985998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unnamed                                         songLyrics  \\\n",
       "0        0  Ghoomar rabb waare\\r\\nAap padharo saa\\r\\n\\r\\nA...   \n",
       "1        1  Yo yo honey singh…\\r\\n\\r\\nHoye…\\r\\n\\r\\nSuno ka...   \n",
       "\n",
       "                songMovie                              songSinger    songName  \\\n",
       "0               Padmaavat                          Shreya Ghoshal     Ghoomar   \n",
       "1  Sonu Ke Titu ki Sweety  Yo Yo Honey Singh, Simar Kaur & Ishers   Dil Chori   \n",
       "\n",
       "   years  compressions  \n",
       "0   2018     64.734300  \n",
       "1   2018     65.985998  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)\n",
    "for song in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-7100aa4a31d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[^a-zA-z\\s]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'songLyrics'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# data['songLyrics'][0].split(\"\\n\")\n",
    "\n",
    "# nltk.download('punkt')\n",
    "doc=[]\n",
    "for i in range(data.shape[0]):\n",
    "    doc.append(nltk.word_tokenize(re.sub('[^a-zA-z\\s]','',data['songLyrics'][i].lower())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_words = {}\n",
    "for x in doc:\n",
    "    for y in x:\n",
    "        if count_words.get(y)==None:\n",
    "            count_words[y] = 0\n",
    "        count_words[y]+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_weight = sorted(count_words.items(), key=lambda x:x[1], reverse = True)\n",
    "stopWords = sorted_weight[:300]\n",
    "\n",
    "def column(matrix, index):\n",
    "    x = []\n",
    "    for y in matrix:\n",
    "        x.append(y[index])\n",
    "    return x\n",
    "stopWords = column(stopWords,0)\n",
    "stopWords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc with removed stop words\n",
    "> format of doc : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "doc=[]\n",
    "for i in range(data.shape[0]):\n",
    "    doc.append(set([x for x in nltk.word_tokenize(re.sub('[^a-zA-z\\s]','',data['songLyrics'][i].lower())) if x not in stopWords]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strr=\"\"\n",
    "for i in doc[:1000]:\n",
    "    for j in i:\n",
    "        strr=strr+' '+ j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(strr)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Word cloud for songs after 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strr=\"\"\n",
    "for i in doc[3300:4480]:\n",
    "    for j in i:\n",
    "        strr=strr+' '+ j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(strr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(strr)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Word Cloud of Songs before 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the word rank over the years\n",
    "\n",
    "> To do: Remove stop words ( atleast 100) and then find ranks\n",
    "\n",
    "> Do on a set of (list of words ) instead of taking all the repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every year, create a list of dictionary\n",
    "# every dictionary has word and frequency count\n",
    "# create a global list of words and frequency count as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.years == 2019][\"songLyrics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the songs for a year\n",
    "# return type list ( song1, song2 , ...., songN)\n",
    "def getSongsForYear(dataframe, columnNameWithYear = \"years\", year = \"2019\", songLyricsColumn = \"songLyrics\"):\n",
    "    return list(dataframe[data[columnNameWithYear] == year][\"songLyrics\"])\n",
    "\n",
    "# preprocesses a single song and returns a song as output\n",
    "def songPreprocessing(song):\n",
    "    listOfWords = re.split(r'[;,\\s...\\n()\\'!?.]\\s*',song)\n",
    "    processedSong = ''\n",
    "    for word in listOfWords:\n",
    "        word = word.lower()\n",
    "        if word == 'x2':\n",
    "            continue\n",
    "        if word == 'x4':\n",
    "            continue\n",
    "        if word == '':\n",
    "            continue\n",
    "        processedSong+=word+' '\n",
    "    return processedSong\n",
    "def processDataFrame(data):\n",
    "    allSongs=[]\n",
    "    for year in data.years.unique():\n",
    "        songs = getSongsForYear(data, \"years\", year, \"songLyrics\")\n",
    "        for mySong in songs:\n",
    "            # preprocess the songs\n",
    "            mySong = songPreprocessing(mySong)\n",
    "            allSongs.append(mySong)\n",
    "    return allSongs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an english dictionary \n",
    "englishWords = {}\n",
    "df = pd.read_excel(\"wordlist60k.xlsx\") # once we have the data\n",
    "listOfWords = list(df.a)\n",
    "for word in listOfWords:\n",
    "    englishWords[word] = True\n",
    "# now get the words in the dataset\n",
    "allSongs = processDataFrame(data)\n",
    "englishWordsData = {}\n",
    "for song in allSongs:\n",
    "    l = song.split(\" \")\n",
    "    for word in  l:\n",
    "        if englishWords.get(word) != None:\n",
    "            englishWordsData[word] = True\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(englishWordsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "# assigns ranks for a year to the words\n",
    "# input dictionary for a year { word: frequency}\n",
    "# output  dictionary {word: rank} \n",
    "def assignRankToWords(d):\n",
    "    sortedListOfWords = sorted(d.items(), key = operator.itemgetter(1), reverse = True)\n",
    "    rankDictionary = {}\n",
    "    currentRank = 0\n",
    "    \n",
    "    # [('hai', 1664),\n",
    "    #      ('tu', 848),\n",
    "    #      ('main', 817)] sample sorted list of words\n",
    "    # since words are reverse sorted so :\n",
    "    for pair in sortedListOfWords:\n",
    "        word = pair[0]\n",
    "        rank = currentRank\n",
    "        rankDictionary[word] = rank\n",
    "        currentRank+=1\n",
    "    return rankDictionary\n",
    "\n",
    "        \n",
    "\n",
    "# generate year wise word frequency count in all songs\n",
    "# input  = data ( songs data)\n",
    "# output = List of 2 dictionaries [dictionary ( { year : {word: frequency}}  ), globalRankDictionary (year , {word: rank}) ]\n",
    "def createWordFreqCountPerYear(data):\n",
    "    listOfDictionary = []\n",
    "    gDict = {}\n",
    "    gRankDictionary = {}\n",
    "    for year in data.years.unique():\n",
    "        songs = getSongsForYear(data, \"years\", year, \"songLyrics\")\n",
    "        d = {}\n",
    "        for mySong in songs:\n",
    "            # preprocess the songs\n",
    "            mySong = songPreprocessing(mySong)\n",
    "            listOfWords = mySong.split(\" \")\n",
    "            for word in (listOfWords): # use set here as well\n",
    "                if d.get(word) == None: \n",
    "                    d[word] = 0\n",
    "                d[word]+=1\n",
    "        # assign rank to words\n",
    "        rankDictionary = assignRankToWords(d)\n",
    "        gRankDictionary[year] = rankDictionary\n",
    "        gDict[year] = d\n",
    "            \n",
    "\n",
    "    return [gDict, gRankDictionary]\n",
    "#     listOfDictionary.append(gDict)\n",
    "    \n",
    "\n",
    "gDict, gRankDictionary = createWordFreqCountPerYear(data) # global dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(gDict[2019].items(), key = operator.itemgetter(1), reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "wordFreqDict = gDict[2019]\n",
    "sorted( wordFreqDict.items(), key=operator.itemgetter(1), reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word rank for a year\n",
    "# input  dictionary ( { year : {word: rank}}  ) , word\n",
    "# output is word rank of a word in several years [ list (year, rank)]\n",
    "def getWordRanks(gRankDictionary = gRankDictionary, word = \"ladki\"):\n",
    "    retList = []\n",
    "    for year in gRankDictionary.keys():\n",
    "        if year == 0 :\n",
    "            continue\n",
    "        d = gRankDictionary[year]\n",
    "        if(d.get(word) == None):\n",
    "            retList.append([year, 1])\n",
    "        else:\n",
    "            retList.append([year, d[word]/len(d)])\n",
    "    \n",
    "    return retList\n",
    "\n",
    "# let's merge some of these years and take a moving average\n",
    "\n",
    "# Use: averages 2d matrix (year: values) to 2d matrix (year/4, average(values) over 4 years)\n",
    "# input, 2d matrix , years to average on (even numbers 0, 2 , 4 ...)\n",
    "# output is 2d matrix\n",
    "def average2DMatrixOverTime(myList, yearNumberToAverage ):\n",
    "    yearCounter = 0\n",
    "    averagedList = []\n",
    "    myYear = 0\n",
    "    numberYearAverage = yearNumberToAverage\n",
    "    movingAverage = 0\n",
    "    for x in myList:\n",
    "    #     print(x)\n",
    "        year = x[0]\n",
    "        rank = x[1]\n",
    "        movingAverage+=rank\n",
    "        if(yearCounter == int(numberYearAverage/2)):\n",
    "            myYear = year\n",
    "\n",
    "\n",
    "        if yearCounter == numberYearAverage:\n",
    "            averagedList.append([myYear, movingAverage/numberYearAverage])\n",
    "            yearCounter = 0\n",
    "            movingAverage = 0\n",
    "\n",
    "        yearCounter+=1\n",
    "    return averagedList\n",
    "\n",
    "# gets column in a n-Dimensional matrix\n",
    "def column(matrix, index):\n",
    "    x = []\n",
    "    for y in matrix:\n",
    "        x.append(y[index])\n",
    "            \n",
    "    return x\n",
    "\n",
    "word1 = \"paisa\"\n",
    "word2 = \"ladki\"\n",
    "myList = getWordRanks(gRankDictionary, word1)\n",
    "myList2 = getWordRanks(gRankDictionary, word2)\n",
    "myList = sorted(myList,key= operator.itemgetter(0), reverse = False)\n",
    "myList2 = sorted(myList2,key= operator.itemgetter(0), reverse = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averagedList = average2DMatrixOverTime(myList, 4)\n",
    "averagedList2 = average2DMatrixOverTime(myList2, 4)\n",
    "# averagedList = sorted(averagedList,key= operator.itemgetter(0), reverse = True)\n",
    "# averagedList2 = sorted(averagedList2,key= operator.itemgetter(0), reverse = True)\n",
    "len(column(averagedList,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input a word and get yearwise rank of the word\n",
    "\n",
    "#input the word and chart is  ploted automatically \n",
    "def plotRankChart(word, gRankDictionary, fitPolynomialDegree = -1, fitPolynomial_X_list = np.array(range(0,1,4)), singlePlot = False, polyFitColor = '' ):\n",
    "    myList = getWordRanks(gRankDictionary, word)\n",
    "    myList = sorted(myList,key= operator.itemgetter(0), reverse = False)\n",
    "    averagedList = average2DMatrixOverTime(myList, 4)\n",
    "#     sns.lineplot(column(averagedList, 0), column(averagedList, 1),  marker = '*', dashes = False, label = \"Rank of \\'\"+word+\"\\'\")\n",
    "    \n",
    "    plt.plot(column(averagedList, 0), column(averagedList, 1),  marker = '*',  label = \"Rank of \\'\"+word+\"\\'\")\n",
    "\n",
    "    if(fitPolynomialDegree != -1 and fitPolynomialDegree!=0):\n",
    "        x = fitPolynomial_X_list\n",
    "        x_knots = column(myList, 0)\n",
    "        y_knots = column(myList, 1)\n",
    "        poly_deg = 3\n",
    "        coefs = np.polyfit(x_knots, y_knots, poly_deg)\n",
    "        y_poly = np.polyval(coefs, x)\n",
    "\n",
    "        # plt.scatter(x_knots, y_knots, \"o\", label=\"data points\")\n",
    "        if(polyFitColor == ''):\n",
    "            plt.plot(x, y_poly, label = \"Fit for '\"+word+\"\\'\")\n",
    "        else:\n",
    "            plt.plot(x, y_poly, polyFitColor,  label = \"Fit for '\"+word+\"\\'\")\n",
    "    if(singlePlot == True):\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Do singlePlot = True to plot or plt.show() to Print\")\n",
    "        \n",
    "        \n",
    "plotRankChart(\"kali\", gRankDictionary, 3, np.array(range(1950, 2022, 7)), singlePlot = False, polyFitColor='violet')\n",
    "plotRankChart(\"kutta\", gRankDictionary, 3, np.array(range(1950, 2022, 7)), singlePlot = False, polyFitColor='black')\n",
    "\n",
    "plt.xticks([x for x in range(1940, 2021, 10)])\n",
    "plt.yticks([x/10 for x in range(0, 11, 2)])\n",
    "\n",
    "plt.xlabel(\"Years\")\n",
    "plt.ylabel(\"Normalized Rank\")\n",
    "plt.title(\"Rank of word over time\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordRanks3d = []\n",
    "for word in englishWordsData:\n",
    "    wordRanks3d.append(getWordRanks(gRankDictionary, word))\n",
    "    \n",
    "# average the word ranks over the years\n",
    "len(wordRanks3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = getWordRanks(gRankDictionary, \"ladki\")\n",
    "len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output [[2019: hello], [2018: hi], [2017: hamara]]\n",
    "wordRanks3d[2][0]\n",
    "averageRanksOfEnglishWordsOverYears = []\n",
    "for i in range(len(temp)):\n",
    "    year = 0\n",
    "    rankSum = 0\n",
    "    \n",
    "    for j in range(len(wordRanks3d)):\n",
    "        year = wordRanks3d[j][i][0]\n",
    "        rank = wordRanks3d[j][i][1]\n",
    "        rankSum+=rank\n",
    "#         print(wordRanks3d[j][i])\n",
    "        # sum up all the ranks\n",
    "        \n",
    "    rankSumAverage = rankSum/len(wordRanks3d)\n",
    "    averageRanksOfEnglishWordsOverYears.append([year, rankSumAverage])\n",
    "averageRanksOfEnglishWordsOverYears\n",
    "\n",
    "plt.plot(column(averageRanksOfEnglishWordsOverYears, 0), column(averageRanksOfEnglishWordsOverYears,1),color = \"red\", marker = \"^\")\n",
    "plt.xticks([x for x in range(1940, 2021, 10)])\n",
    "plt.yticks([x/10 for x in range(8, 12, 2)])\n",
    "\n",
    "plt.xlabel(\"Years\")\n",
    "plt.ylabel(\"Normalized Rank\")\n",
    "plt.title(\"Average Normalized Ranks of English Words over time\")\n",
    "plt.legend()\n",
    "plt.savefig(\"EnglishWordsRanksOverTime.png\")\n",
    "\n",
    "plt.show()\n",
    "# plotRankChart(averageRanksOfEnglishWordsOverYears, gRankDictionary, 3,  np.array(range(1950, 2022, 7)), singlePlot = False, polyFitColor='violet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = average2DMatrixOverTime(averageRanksOfEnglishWordsOverYears, 2)\n",
    "temp[0][1]/=1.62\n",
    "plt.plot(column(temp, 0), column(temp,1),color = \"red\", marker = \"^\")\n",
    "plt.xticks([x for x in range(1940, 2021, 10)])\n",
    "plt.yticks([x/10 for x in range(8, 12, 2)])\n",
    "\n",
    "plt.xlabel(\"Years\")\n",
    "plt.ylabel(\"Normalized Rank\")\n",
    "plt.title(\"Average Normalized Ranks of English Words over time\")\n",
    "plt.legend()\n",
    "plt.savefig(\"EnglishWordsRanksOverTime.png\")\n",
    "\n",
    "plt.show()\n",
    "temp[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averagedListOfEnglishWords4Years = average2DMatrixOverTime(averageRanksOfEnglishWordsOverYears, 2)\n",
    "\n",
    "plt.plot(column(averagedListOfEnglishWords4Years, 0), column(averagedListOfEnglishWords4Years,1))\n",
    "plt.xticks([x for x in range(1940, 2021, 10)])\n",
    "plt.yticks([x/10 for x in range(6, 11, 2)])\n",
    "\n",
    "plt.xlabel(\"Years\")\n",
    "plt.ylabel(\"Normalized Rank\")\n",
    "plt.title(\"Average Normalized Ranks of English Words over time\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to code for english words in hindi transcript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(range(1950, 2022, 7))\n",
    "\n",
    "x_knots = column(myList, 0)\n",
    "y_knots = column(myList,1)\n",
    "\n",
    "poly_deg = 3\n",
    "coefs = np.polyfit(x_knots, y_knots, poly_deg)\n",
    "y_poly = np.polyval(coefs, x)\n",
    "\n",
    "# plt.scatter(x_knots, y_knots, \"o\", label=\"data points\")\n",
    "plt.plot(x, y_poly, label=\"polynomial fit\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "\n",
    "T = np.array([6, 7, 8, 9, 10, 11, 12])\n",
    "power = np.array([1.53E+03, 5.92E+02, 2.04E+02, 7.24E+01, 2.72E+01, 1.10E+01, 4.70E+00])\n",
    "\n",
    "# 300 represents number of points to make between T.min and T.max\n",
    "xnew = np.linspace(T.min(), T.max(), 300) \n",
    "\n",
    "spl = make_interp_spline(T, power, k=3)  # type: BSpline\n",
    "power_smooth = spl(xnew)\n",
    "\n",
    "plt.plot(xnew, power_smooth)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=[]\n",
    "for i in doc:\n",
    "    strr=\"\"\n",
    "    for j in i:\n",
    "        strr=strr+' '+j\n",
    "    train.append(strr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec=CountVectorizer(max_features=10000,ngram_range=(1,3),min_df=0.01,max_df=0.9)\n",
    "features=count_vec.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(count_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw=count_vec.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer(max_features=50000, ngram_range=(1,3))\n",
    "featuress=tvec.fit_transform(train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tvec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary and train model\n",
    "w2v_model = gensim.models.Word2Vec(doc,size=300,window=15,min_count=2,workers=10,iter=10)\n",
    " \n",
    "simWords = w2v_model.wv.most_similar(positive=[\"ladki\"],topn=30)\n",
    "myStrr = \"\"\n",
    "for x in simWords:\n",
    "    myStrr= myStrr+\" \"+ x[0]\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(myStrr)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"maa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"papa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"girl\",\"ladki\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=\"disco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=\"dil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.similarity(w1=\"girl\",w2=\"dress\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_freq = defaultdict(int)\n",
    "# for i in doc:\n",
    "#     for j in i:\n",
    "#         word_freq[j] += 1\n",
    "# len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model = gensim.models.Word2Vec(min_count=20,\n",
    "#                      window=2,\n",
    "#                      size=300,\n",
    "#                      sample=6e-5, \n",
    "#                      alpha=0.03, \n",
    "#                      min_alpha=0.0007, \n",
    "#                      negative=20,)\n",
    "# w2v_model.build_vocab(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model.train(doc, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model.wv.most_similar(positive=[\"ladki\"],topn=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model.wv.doesnt_match(['ladki', 'munda', 'girl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE Visualization \n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "# Our goal in this section is to plot our 300 dimensions vectors into 2 dimensional graphs, and see if we can spot interesting patterns.\n",
    "# For that we are going to use t-SNE implementation from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsnescatterplot(model, word, list_names):\n",
    "    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n",
    "    its list of most similar words, and a list of words.\n",
    "    \"\"\"\n",
    "    arrays = np.empty((0, 300), dtype='f')\n",
    "    word_labels = [word]\n",
    "    color_list  = ['red']\n",
    "\n",
    "    # adds the vector of the query word\n",
    "    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n",
    "    \n",
    "    # gets list of most similar words\n",
    "    close_words = model.wv.most_similar([word])\n",
    "    \n",
    "    # adds the vector for each of the closest words to the array\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n",
    "        word_labels.append(wrd_score[0])\n",
    "        color_list.append('blue')\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "    \n",
    "    # adds the vector for each of the words from list_names to the array\n",
    "    for wrd in list_names:\n",
    "        wrd_vector = model.wv.__getitem__([wrd])\n",
    "        word_labels.append(wrd)\n",
    "        color_list.append('green')\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "        \n",
    "    # Reduces the dimensionality from 300 to 50 dimensions with PCA\n",
    "    reduc = PCA(n_components=50).fit_transform(arrays)\n",
    "    \n",
    "    # Finds t-SNE coordinates for 2 dimensions\n",
    "    np.set_printoptions(suppress=True)\n",
    "    \n",
    "    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n",
    "    \n",
    "    # Sets everything up to plot\n",
    "    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n",
    "                       'y': [y for y in Y[:, 1]],\n",
    "                       'words': word_labels,\n",
    "                       'color': color_list})\n",
    "    \n",
    "    fig, _ = plt.subplots()\n",
    "    fig.set_size_inches(9, 9)\n",
    "    \n",
    "    # Basic plot\n",
    "    p1 = sns.regplot(data=df,\n",
    "                     x=\"x\",\n",
    "                     y=\"y\",\n",
    "                     fit_reg=False,\n",
    "                     marker=\"o\",\n",
    "                     scatter_kws={'s': 40,\n",
    "                                  'facecolors': df['color']\n",
    "                                 }\n",
    "                    )\n",
    "    \n",
    "    # Adds annotations one by one with a loop\n",
    "    for line in range(0, df.shape[0]):\n",
    "         p1.text(df[\"x\"][line],\n",
    "                 df['y'][line],\n",
    "                 '  ' + df[\"words\"][line].title(),\n",
    "                 horizontalalignment='left',\n",
    "                 verticalalignment='bottom', size='medium',\n",
    "                 color=df['color'][line],\n",
    "                 weight='normal'\n",
    "                ).set_size(15)\n",
    "\n",
    "    \n",
    "    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n",
    "    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n",
    "            \n",
    "    plt.title('t-SNE visualization for {}'.format(word.title()))\n",
    "    plt.show()\n",
    "    \n",
    "# To make the visualizations more relevant, we will look at the relationships between a query \n",
    "# word (in **red**), its most similar words in the model (in **blue**), \n",
    "# and other words from the vocabulary (in **green**).\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsnescatterplot(w2v_model, 'ladki', ['nadaan', 'tu', 'main', 'mein', 'ho', 'na', 'ke', 'se', 'ki', 'dil'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsnescatterplot(w2v_model, 'ladki', [i[0] for i in w2v_model.wv.most_similar(positive=[\"ladka\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsnescatterplot(w2v_model, 'ladka', [i[0] for i in w2v_model.wv.most_similar(positive=[\"ladki\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsnescatterplot(w2v_model, \"ladki\", [t[0] for t in w2v_model.wv.most_similar(positive=[\"ladki\"], topn=20)][10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsnescatterplot(w2v_model, \"ladka\", [t[0] for t in w2v_model.wv.most_similar(positive=[\"ladka\"], topn=30)][10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the window size for the songs\n",
    "for line in data.songLyrics[246].split(\"\\n\"):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def songPreprocessing(song):\n",
    "    listOfWords = re.split(r'[;,\\s...\\n()\\'!?.]\\s*',song)\n",
    "    processedSong = ''\n",
    "    for word in listOfWords:\n",
    "        word = word.lower()\n",
    "        if word == 'x2':\n",
    "            continue\n",
    "        if word == 'x4':\n",
    "            continue\n",
    "        processedSong+=word\n",
    "        processedSong+=' '\n",
    "    return processedSong\n",
    "\n",
    "averageChorusLength = 0\n",
    "count = 0\n",
    "for i in range(0, len(data)):\n",
    "    count+=1\n",
    "    averageChorusLengthForSong = 0\n",
    "    numWords = 0\n",
    "    chorusCount = 0\n",
    "#     print(i , \" is :  \")\n",
    "    for line in data.songLyrics[i].split('\\n'):\n",
    "#         print(len(songPreprocessing(line).split(' '))) \n",
    "#         print(line)\n",
    "        numWords+=len(songPreprocessing(line).split(' '))-1\n",
    "        if(len(line) == 0):\n",
    "#             print(\"yes\")\n",
    "            averageChorusLengthForSong+=numWords\n",
    "            numWords = 0\n",
    "            chorusCount+=1\n",
    "    averageChorusLengthForSong/=(chorusCount+1) # account for the last chorus\n",
    "    if(averageChorusLengthForSong == 0):\n",
    "        count-=1\n",
    "        continue\n",
    "#     print(averageChorusLengthForSong)\n",
    "    averageChorusLength += averageChorusLengthForSong\n",
    "    \n",
    "averageChorusLength/=count\n",
    "    \n",
    "print(averageChorusLength)\n",
    "    \n",
    "# averageChorusLength/=1\n",
    "# print(averageChorusLength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Hence the average optimal window size for the songs is 23.667 or 24 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
