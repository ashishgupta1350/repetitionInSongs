{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"mergedData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>4385.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>2009.03626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>32.88715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>2011.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>2013.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>2016.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>2019.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            years\n",
       "count  4385.00000\n",
       "mean   2009.03626\n",
       "std      32.88715\n",
       "min       0.00000\n",
       "25%    2011.00000\n",
       "50%    2013.00000\n",
       "75%    2016.00000\n",
       "max    2019.00000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieName</th>\n",
       "      <th>songName</th>\n",
       "      <th>songSinger</th>\n",
       "      <th>songMusic</th>\n",
       "      <th>songLyricist</th>\n",
       "      <th>songLyrics</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Bala</td>\n",
       "      <td>Don’t Be Shy</td>\n",
       "      <td>Badshah, Shalmali Kholgade, Gurdeep Mehendi</td>\n",
       "      <td>Sachin-Jigar</td>\n",
       "      <td>Mellow D, Badshah</td>\n",
       "      <td>Sun, main hoon thoda sanki\\r\\nKarun mann ki\\r\\...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Laal Kaptaan</td>\n",
       "      <td>Lahu Ka Rang Kara</td>\n",
       "      <td>Samira Koppikar</td>\n",
       "      <td>Samira Koppikar</td>\n",
       "      <td>Sahib</td>\n",
       "      <td>Morey.. lahu ka rang kara\\r\\nMorey lahu ka ran...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Laal Kaptaan</td>\n",
       "      <td>Red Red Najariya</td>\n",
       "      <td>Shreya Ghoshal</td>\n",
       "      <td>Samira Koppikar</td>\n",
       "      <td>Saurabh Jain</td>\n",
       "      <td>Badnaam shehar, badnaam gully\\r\\nIsme har raat...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Laal Kaptaan</td>\n",
       "      <td>Kaal Kaal</td>\n",
       "      <td>Brijesh Shandilya, Dino James</td>\n",
       "      <td>Samira Koppikar</td>\n",
       "      <td>Saurabh Jain</td>\n",
       "      <td>Kaal kaal, kaal kaal, jo sapaat chal raha\\r\\nW...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Laal Kaptaan</td>\n",
       "      <td>Taandav</td>\n",
       "      <td>Kailash Kher, Brijesh Shandilya</td>\n",
       "      <td>Samira Koppikar</td>\n",
       "      <td>Puneet Sharma</td>\n",
       "      <td>Shor hai andher mein\\r\\nJo dher murda pedon ka...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      movieName           songName  \\\n",
       "0          Bala       Don’t Be Shy   \n",
       "1  Laal Kaptaan  Lahu Ka Rang Kara   \n",
       "2  Laal Kaptaan   Red Red Najariya   \n",
       "3  Laal Kaptaan          Kaal Kaal   \n",
       "4  Laal Kaptaan            Taandav   \n",
       "\n",
       "                                    songSinger        songMusic  \\\n",
       "0  Badshah, Shalmali Kholgade, Gurdeep Mehendi     Sachin-Jigar   \n",
       "1                              Samira Koppikar  Samira Koppikar   \n",
       "2                               Shreya Ghoshal  Samira Koppikar   \n",
       "3                Brijesh Shandilya, Dino James  Samira Koppikar   \n",
       "4              Kailash Kher, Brijesh Shandilya  Samira Koppikar   \n",
       "\n",
       "        songLyricist                                         songLyrics  years  \n",
       "0  Mellow D, Badshah  Sun, main hoon thoda sanki\\r\\nKarun mann ki\\r\\...   2019  \n",
       "1              Sahib  Morey.. lahu ka rang kara\\r\\nMorey lahu ka ran...   2019  \n",
       "2       Saurabh Jain  Badnaam shehar, badnaam gully\\r\\nIsme har raat...   2019  \n",
       "3       Saurabh Jain  Kaal kaal, kaal kaal, jo sapaat chal raha\\r\\nW...   2019  \n",
       "4      Puneet Sharma  Shor hai andher mein\\r\\nJo dher murda pedon ka...   2019  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "dataset = ''\n",
    "countOfWords = 0\n",
    "uniqueWords = {}\n",
    "for song in list(df.songLyrics):\n",
    "    listOfWords = re.split(r'[;,\\s...\\n()\\'!?.]\\s*',song) # gets me a list of words\n",
    "    for word in listOfWords:\n",
    "#         if word ==  '' || word=='(' || word==')' || word =='\\'':\n",
    "#             pass\n",
    "#         else:\n",
    "        word = word.lower()\n",
    "        if word == 'x2':\n",
    "            continue\n",
    "        if word == 'x4':\n",
    "            continue\n",
    "        dataset+=' '+word\n",
    "        countOfWords+=1\n",
    "        uniqueWords[word] = 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> \n",
      " doesnot exist\n",
      "\n",
      "saanu -> sa\n",
      "saanu doesnot exist\n",
      "\n",
      "kare -> kar\n",
      "kare doesnot exist\n",
      "\n",
      "ishaare -> ishaar\n",
      "ishaare doesnot exist\n",
      "\n",
      "touch -> touch\n",
      "touch doesnot exist\n",
      "\n",
      "my -> my\n",
      "my doesnot exist\n",
      "\n",
      "don -> do\n",
      "don doesnot exist\n",
      "\n",
      "t -> t\n",
      "t doesnot exist\n",
      "\n",
      "be -> be\n",
      "be doesnot exist\n",
      "\n",
      "shy -> sh\n",
      "shy doesnot exist\n",
      "\n",
      "honey -> ho\n",
      "honey doesnot exist\n",
      "\n",
      "fly -> fl\n",
      "fly doesnot exist\n",
      "\n",
      "befikar -> befikar\n",
      "befikar doesnot exist\n",
      "\n",
      "kuch -> kuch\n",
      "kuch doesnot exist\n",
      "\n",
      "bole -> bol\n",
      "bole doesnot exist\n",
      "\n",
      "bina -> bi\n",
      "bina doesnot exist\n",
      "\n",
      "aankhon -> aankh\n",
      "aankhon doesnot exist\n",
      "\n",
      "se -> se\n",
      "se doesnot exist\n",
      "\n",
      "shaitani -> shait\n",
      "shaitani doesnot exist\n",
      "\n",
      "jaan -> ja\n",
      "jaan doesnot exist\n",
      "\n",
      "bujhke -> bujhk\n",
      "bujhke doesnot exist\n",
      "\n",
      "mere -> mer\n",
      "mere doesnot exist\n",
      "\n",
      "saath -> saath\n",
      "saath doesnot exist\n",
      "\n",
      "tujhko -> tujhk\n",
      "tujhko doesnot exist\n",
      "\n",
      "dekhe -> dekh\n",
      "dekhe doesnot exist\n",
      "\n",
      "mera -> mer\n",
      "mera doesnot exist\n",
      "\n",
      "ab -> ab\n",
      "ab doesnot exist\n",
      "\n",
      "kahin -> kah\n",
      "kahin doesnot exist\n",
      "\n",
      "lage -> la\n",
      "lage doesnot exist\n",
      "\n",
      "husn -> hus\n",
      "husn doesnot exist\n",
      "\n",
      "pe -> pe\n",
      "pe doesnot exist\n",
      "\n",
      "aankhein -> aankh\n",
      "aankhein doesnot exist\n",
      "\n",
      "sek -> sek\n",
      "sek doesnot exist\n",
      "\n",
      "kar -> kar\n",
      "kar doesnot exist\n",
      "\n",
      "thake -> thak\n",
      "thake doesnot exist\n",
      "\n",
      "upar -> upar\n",
      "upar doesnot exist\n",
      "\n",
      "raat -> raat\n",
      "raat doesnot exist\n",
      "\n",
      "hoti -> hot\n",
      "hoti doesnot exist\n",
      "\n",
      "ja -> ja\n",
      "ja doesnot exist\n",
      "\n",
      "rahi -> rah\n",
      "rahi doesnot exist\n",
      "\n",
      "naughty -> naught\n",
      "naughty doesnot exist\n",
      "\n",
      "morey -> mor\n",
      "morey doesnot exist\n",
      "\n",
      "lahu -> lah\n",
      "lahu doesnot exist\n",
      "\n",
      "ka -> ka\n",
      "ka doesnot exist\n",
      "\n",
      "rang -> ra\n",
      "rang doesnot exist\n",
      "\n",
      "kara -> kar\n",
      "kara doesnot exist\n",
      "\n",
      "kaare -> kaar\n",
      "kaare doesnot exist\n",
      "\n",
      "badarwa -> badarw\n",
      "badarwa doesnot exist\n",
      "\n",
      "manwa -> manw\n",
      "manwa doesnot exist\n",
      "\n",
      "ghoome -> ghoom\n",
      "ghoome doesnot exist\n",
      "\n",
      "bhujanga -> bhuj\n",
      "bhujanga doesnot exist\n",
      "\n",
      "sapno -> sap\n",
      "sapno doesnot exist\n",
      "\n",
      "jhoome -> jhoom\n",
      "jhoome doesnot exist\n",
      "\n",
      "ghole -> ghol\n",
      "ghole doesnot exist\n",
      "\n",
      "zehar -> zehar\n",
      "zehar doesnot exist\n",
      "\n",
      "dhaara -> dhaar\n",
      "dhaara doesnot exist\n",
      "\n",
      "kaara -> kaar\n",
      "kaara doesnot exist\n",
      "\n",
      "katit -> katit\n",
      "katit doesnot exist\n",
      "\n",
      "kaadank -> kaadank\n",
      "kaadank doesnot exist\n",
      "\n",
      "kapat -> kapat\n",
      "kapat doesnot exist\n",
      "\n",
      "kalank -> kalank\n",
      "kalank doesnot exist\n",
      "\n",
      "maathe -> maath\n",
      "maathe doesnot exist\n",
      "\n",
      "mandha -> mandh\n",
      "mandha doesnot exist\n",
      "\n",
      "koi -> ko\n",
      "koi doesnot exist\n",
      "\n",
      "shraap -> shraap\n",
      "shraap doesnot exist\n",
      "\n",
      "jeevan -> jeev\n",
      "jeevan doesnot exist\n",
      "\n",
      "raja -> raj\n",
      "raja doesnot exist\n",
      "\n",
      "ya -> ya\n",
      "ya doesnot exist\n",
      "\n",
      "rank -> rank\n",
      "rank doesnot exist\n",
      "\n",
      "kabila -> kabil\n",
      "kabila doesnot exist\n",
      "\n",
      "jhund -> jhund\n",
      "jhund doesnot exist\n",
      "\n",
      "har -> har\n",
      "har doesnot exist\n",
      "\n",
      "praan -> pr\n",
      "praan doesnot exist\n",
      "\n",
      "lakshya -> laksh\n",
      "lakshya doesnot exist\n",
      "\n",
      "maran -> mar\n",
      "maran doesnot exist\n",
      "\n",
      "jeetna -> jeet\n",
      "jeetna doesnot exist\n",
      "\n",
      "jo -> jo\n",
      "jo doesnot exist\n",
      "\n",
      "rann -> ra\n",
      "rann doesnot exist\n",
      "\n",
      "bairi -> bair\n",
      "bairi doesnot exist\n",
      "\n",
      "daman -> dam\n",
      "daman doesnot exist\n",
      "\n",
      "warna -> war\n",
      "warna doesnot exist\n",
      "\n",
      "mile -> mil\n",
      "mile doesnot exist\n",
      "\n",
      "chhutkaara -> chhutkaar\n",
      "chhutkaara doesnot exist\n",
      "\n",
      "aa -> aa\n",
      "aa doesnot exist\n",
      "\n",
      "badnaam -> badnaam\n",
      "badnaam doesnot exist\n",
      "\n",
      "shehar -> shehar\n",
      "shehar doesnot exist\n",
      "\n",
      "gully -> gull\n",
      "gully doesnot exist\n",
      "\n",
      "isme -> ism\n",
      "isme doesnot exist\n",
      "\n",
      "dhoop -> dhoop\n",
      "dhoop doesnot exist\n",
      "\n",
      "khili -> khil\n",
      "khili doesnot exist\n",
      "\n",
      "dhali -> dhal\n",
      "dhali doesnot exist\n",
      "\n",
      "khoob -> khoob\n",
      "khoob doesnot exist\n",
      "\n",
      "chali -> chal\n",
      "chali doesnot exist\n",
      "\n",
      "red -> red\n",
      "red doesnot exist\n",
      "\n",
      "najariya -> najar\n",
      "najariya doesnot exist\n",
      "\n",
      "arey -> ar\n",
      "arey doesnot exist\n",
      "\n",
      "bada -> bad\n",
      "bada doesnot exist\n",
      "\n",
      "jaalim -> jaalim\n",
      "jaalim doesnot exist\n",
      "\n",
      "kaptaan -> kapt\n",
      "kaptaan doesnot exist\n",
      "\n",
      "re -> re\n",
      "re doesnot exist\n",
      "\n",
      "jalim -> jalim\n",
      "jalim doesnot exist\n",
      "\n",
      "ghaav -> ghaav\n",
      "ghaav doesnot exist\n",
      "\n",
      "deve -> dev\n",
      "deve doesnot exist\n",
      "\n",
      "karajwa -> karajw\n",
      "karajwa doesnot exist\n",
      "\n",
      "par -> par\n",
      "par doesnot exist\n",
      "\n",
      "dhyaan -> dh\n",
      "dhyaan doesnot exist\n",
      "\n",
      "haan -> ha\n",
      "haan doesnot exist\n",
      "\n",
      "bahut -> bahut\n",
      "bahut doesnot exist\n",
      "\n",
      "aate -> aat\n",
      "aate doesnot exist\n",
      "\n",
      "hain -> ha\n",
      "hain doesnot exist\n",
      "\n",
      "iss -> iss\n",
      "iss doesnot exist\n",
      "\n",
      "jungle -> jungl\n",
      "jungle doesnot exist\n",
      "\n",
      "aisa -> ais\n",
      "aisa doesnot exist\n",
      "\n",
      "aave -> aav\n",
      "aave doesnot exist\n",
      "\n",
      "sigdi -> sigd\n",
      "sigdi doesnot exist\n",
      "\n",
      "maddham -> maddham\n",
      "maddham doesnot exist\n",
      "\n",
      "si -> si\n",
      "si doesnot exist\n",
      "\n",
      "haaye -> ha\n",
      "haaye doesnot exist\n",
      "\n",
      "aag -> aa\n",
      "aag doesnot exist\n",
      "\n",
      "jalaave -> jalaav\n",
      "jalaave doesnot exist\n",
      "\n",
      "do -> do\n",
      "do doesnot exist\n",
      "\n",
      "pal -> pal\n",
      "pal doesnot exist\n",
      "\n",
      "ko -> ko\n",
      "ko doesnot exist\n",
      "\n",
      "nain -> na\n",
      "nain doesnot exist\n",
      "\n",
      "milave -> milav\n",
      "milave doesnot exist\n",
      "\n",
      "toh -> toh\n",
      "toh doesnot exist\n",
      "\n",
      "sab -> sab\n",
      "sab doesnot exist\n",
      "\n",
      "kurbaan -> kurb\n",
      "kurbaan doesnot exist\n",
      "\n",
      "kaal -> kaal\n",
      "kaal doesnot exist\n",
      "\n",
      "sapaat -> sapaat\n",
      "sapaat doesnot exist\n",
      "\n",
      "chal -> chal\n",
      "chal doesnot exist\n",
      "\n",
      "raha -> rah\n",
      "raha doesnot exist\n",
      "\n",
      "wo -> wo\n",
      "wo doesnot exist\n",
      "\n",
      "gol -> gol\n",
      "gol doesnot exist\n",
      "\n",
      "duniya -> du\n",
      "duniya doesnot exist\n",
      "\n",
      "sadiyon -> sad\n",
      "sadiyon doesnot exist\n",
      "\n",
      "woh -> woh\n",
      "woh doesnot exist\n",
      "\n",
      "ek -> ek\n",
      "ek doesnot exist\n",
      "\n",
      "mashaal -> mashaal\n",
      "mashaal doesnot exist\n",
      "\n",
      "aadmi -> aadm\n",
      "aadmi doesnot exist\n",
      "\n",
      "bandar -> bandar\n",
      "bandar doesnot exist\n",
      "\n",
      "sa -> sa\n",
      "sa doesnot exist\n",
      "\n",
      "banke -> bank\n",
      "banke doesnot exist\n",
      "\n",
      "sikandar -> sikandar\n",
      "sikandar doesnot exist\n",
      "\n",
      "neetiyon -> neet\n",
      "neetiyon doesnot exist\n",
      "\n",
      "dambh -> dambh\n",
      "dambh doesnot exist\n",
      "\n",
      "roz -> roz\n",
      "roz doesnot exist\n",
      "\n",
      "bharta -> bhart\n",
      "bharta doesnot exist\n",
      "\n",
      "peedhi -> peedh\n",
      "peedhi doesnot exist\n",
      "\n",
      "umra -> umr\n",
      "umra doesnot exist\n",
      "\n",
      "seedhi -> seedh\n",
      "seedhi doesnot exist\n",
      "\n",
      "chadhta -> chadht\n",
      "chadhta doesnot exist\n",
      "\n",
      "phisalta -> phisalt\n",
      "phisalta doesnot exist\n",
      "\n",
      "aham -> aham\n",
      "aham doesnot exist\n",
      "\n",
      "jeeta -> jeet\n",
      "jeeta doesnot exist\n",
      "\n",
      "kis -> kis\n",
      "kis doesnot exist\n",
      "\n",
      "vaham -> vaham\n",
      "vaham doesnot exist\n",
      "\n",
      "rakt -> rakt\n",
      "rakt doesnot exist\n",
      "\n",
      "kyun -> ky\n",
      "kyun doesnot exist\n",
      "\n",
      "uske -> usk\n",
      "uske doesnot exist\n",
      "\n",
      "ubaal -> ubaal\n",
      "ubaal doesnot exist\n",
      "\n",
      "khatm -> khatm\n",
      "khatm doesnot exist\n",
      "\n",
      "teri -> ter\n",
      "teri doesnot exist\n",
      "\n",
      "laalsa -> laals\n",
      "laalsa doesnot exist\n",
      "\n",
      "jaane -> ja\n",
      "jaane doesnot exist\n",
      "\n",
      "samay -> sam\n",
      "samay doesnot exist\n",
      "\n",
      "bhale -> bhal\n",
      "bhale doesnot exist\n",
      "\n",
      "taalta -> taalt\n",
      "taalta doesnot exist\n",
      "\n",
      "karega -> kar\n",
      "karega doesnot exist\n",
      "\n",
      "kya -> ky\n",
      "kya doesnot exist\n",
      "\n",
      "murjhati -> murjhat\n",
      "murjhati doesnot exist\n",
      "\n",
      "khaal -> khaal\n",
      "khaal doesnot exist\n",
      "\n",
      "bas -> bas\n",
      "bas doesnot exist\n",
      "\n",
      "khel -> khel\n",
      "khel doesnot exist\n",
      "\n",
      "saaya -> sa\n",
      "saaya doesnot exist\n",
      "\n",
      "saare -> saar\n",
      "saare doesnot exist\n",
      "\n",
      "bramhaand -> bramhaand\n",
      "bramhaand doesnot exist\n",
      "\n",
      "teer -> teer\n",
      "teer doesnot exist\n",
      "\n",
      "vinash -> vinash\n",
      "vinash doesnot exist\n",
      "\n",
      "kamaan -> kam\n",
      "kamaan doesnot exist\n",
      "\n",
      "deta -> det\n",
      "deta doesnot exist\n",
      "\n",
      "bhar -> bhar\n",
      "bhar doesnot exist\n",
      "\n",
      "saansein -> saans\n",
      "saansein doesnot exist\n",
      "\n",
      "pratyaksh -> pratyaksh\n",
      "pratyaksh doesnot exist\n",
      "\n",
      "khada -> khad\n",
      "khada doesnot exist\n",
      "\n",
      "pramaan -> pram\n",
      "pramaan doesnot exist\n",
      "\n",
      "ajar -> ajar\n",
      "ajar doesnot exist\n",
      "\n",
      "amar -> amar\n",
      "amar doesnot exist\n",
      "\n",
      "anaadi -> anaad\n",
      "anaadi doesnot exist\n",
      "\n",
      "ant -> ant\n",
      "ant doesnot exist\n",
      "\n",
      "granth -> granth\n",
      "granth doesnot exist\n",
      "\n",
      "dharm -> dharm\n",
      "dharm doesnot exist\n",
      "\n",
      "uska -> usk\n",
      "uska doesnot exist\n",
      "\n",
      "shadyantra -> shadyantr\n",
      "shadyantra doesnot exist\n",
      "\n",
      "gada -> gad\n",
      "gada doesnot exist\n",
      "\n",
      "chaatiyon -> chaat\n",
      "chaatiyon doesnot exist\n",
      "\n",
      "shool -> shool\n",
      "shool doesnot exist\n",
      "\n",
      "usko -> usk\n",
      "usko doesnot exist\n",
      "\n",
      "bhoolna -> bhool\n",
      "bhoolna doesnot exist\n",
      "\n",
      "bhool -> bhool\n",
      "bhool doesnot exist\n",
      "\n",
      "isi -> is\n",
      "isi doesnot exist\n",
      "\n",
      "ke -> ke\n",
      "ke doesnot exist\n",
      "\n",
      "maare -> maar\n",
      "maare doesnot exist\n",
      "\n",
      "haare -> haar\n",
      "haare doesnot exist\n",
      "\n",
      "isko -> isk\n",
      "isko doesnot exist\n",
      "\n",
      "jeet -> jeet\n",
      "jeet doesnot exist\n",
      "\n",
      "le -> le\n",
      "le doesnot exist\n",
      "\n",
      "mahakaal -> mahakaal\n",
      "mahakaal doesnot exist\n",
      "\n",
      "shor -> shor\n",
      "shor doesnot exist\n",
      "\n",
      "andher -> andher\n",
      "andher doesnot exist\n",
      "\n",
      "dher -> dher\n",
      "dher doesnot exist\n",
      "\n",
      "murda -> murd\n",
      "murda doesnot exist\n",
      "\n",
      "pedon -> ped\n",
      "pedon doesnot exist\n",
      "\n",
      "sulag -> sul\n",
      "sulag doesnot exist\n",
      "\n",
      "gaya -> ga\n",
      "gaya doesnot exist\n",
      "\n",
      "jhulas -> jhulas\n",
      "jhulas doesnot exist\n",
      "\n",
      "lo -> lo\n",
      "lo doesnot exist\n",
      "\n",
      "jwala -> jwal\n",
      "jwala doesnot exist\n",
      "\n",
      "kaath -> kaath\n",
      "kaath doesnot exist\n",
      "\n",
      "pher -> pher\n",
      "pher doesnot exist\n",
      "\n",
      "jeev -> jeev\n",
      "jeev doesnot exist\n",
      "\n",
      "janm -> janm\n",
      "janm doesnot exist\n",
      "\n",
      "jal -> jal\n",
      "jal doesnot exist\n",
      "\n",
      "dhuaan -> dh\n",
      "dhuaan doesnot exist\n",
      "\n",
      "hua -> hu\n",
      "hua doesnot exist\n",
      "\n",
      "swaha -> swah\n",
      "swaha doesnot exist\n",
      "\n",
      "raakh -> raakh\n",
      "raakh doesnot exist\n",
      "\n",
      "damruon -> damr\n",
      "damruon doesnot exist\n",
      "\n",
      "damdamata -> damdamat\n",
      "damdamata doesnot exist\n",
      "\n",
      "krodh -> krodh\n",
      "krodh doesnot exist\n",
      "\n",
      "naache -> naach\n",
      "naache doesnot exist\n",
      "\n",
      "krudh -> krudh\n",
      "krudh doesnot exist\n",
      "\n",
      "mastakon -> mastak\n",
      "mastakon doesnot exist\n",
      "\n",
      "maaya -> ma\n",
      "maaya doesnot exist\n",
      "\n",
      "chat -> chat\n",
      "chat doesnot exist\n",
      "\n",
      "chatakati -> chatakat\n",
      "chatakati doesnot exist\n",
      "\n",
      "khat -> khat\n",
      "khat doesnot exist\n",
      "\n",
      "khatakati -> khatakat\n",
      "khatakati doesnot exist\n",
      "\n",
      "bhat -> bhat\n",
      "bhat doesnot exist\n",
      "\n",
      "bhatakati -> bhatakat\n",
      "bhatakati doesnot exist\n",
      "\n",
      "naachti -> naacht\n",
      "naachti doesnot exist\n",
      "\n",
      "chhatiyon -> chhat\n",
      "chhatiyon doesnot exist\n",
      "\n",
      "traahi -> traah\n",
      "traahi doesnot exist\n",
      "\n",
      "dhan-dhankati -> dhan-dhankat\n",
      "dhan-dhankati doesnot exist\n",
      "\n",
      "jhan-jhanakati -> jhan-jhanakat\n",
      "jhan-jhanakati doesnot exist\n",
      "\n",
      "sulagati -> sulagat\n",
      "sulagati doesnot exist\n",
      "\n",
      "traasdi -> traasd\n",
      "traasdi doesnot exist\n",
      "\n",
      "yojnaayein -> yoj\n",
      "yojnaayein doesnot exist\n",
      "\n",
      "yam -> yam\n",
      "yam doesnot exist\n",
      "\n",
      "banata -> banat\n",
      "banata doesnot exist\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dataset[:1000]\n",
    "import re\n",
    "uniqueWords.keys()\n",
    "ourWords = list(uniqueWords.keys())[50:300]\n",
    "def stemmer(word):\n",
    "    return re.sub(r'(.{2,}?)([aeiougyn]+$)',r'\\1', word)\n",
    "\n",
    "for word in ourWords:\n",
    "    print(word + \" -> \" + stemmer(word))\n",
    "    \n",
    "    try:\n",
    "        if model2.similarity(word, stemmer(word))> 0.4:\n",
    "           print(word + \" -> \" + stemmer(word))\n",
    "    except:\n",
    "        print(word + \" doesnot exist\")\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "870630"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40866"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uniqueWords.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "listDataset = list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(dataset, min_count = 1, size = 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "dataset_gensim = []\n",
    "countOfWords = 0\n",
    "uniqueWords = {}\n",
    "for song in list(df.songLyrics):\n",
    "    listOfWords = re.split(r'[;,\\s...\\n()\\'!?.]\\s*',song) # gets me a list of words\n",
    "    wordList = []\n",
    "    for word in listOfWords:\n",
    "#         if word ==  '' || word=='(' || word==')' || word =='\\'':\n",
    "#             pass\n",
    "#         else:\n",
    "        word = word.lower()\n",
    "        if word == 'x2':\n",
    "            continue\n",
    "        if word == 'x4':\n",
    "            continue\n",
    "        wordList.append(word)\n",
    "        countOfWords+=1\n",
    "        uniqueWords[word] = 1\n",
    "    dataset_gensim.append(wordList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(dataset_gensim, min_count = 1, size = 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('rakshak', 0.9684677720069885),\n",
       " ('12', 0.9616156816482544),\n",
       " ('oonche', 0.9593517184257507),\n",
       " ('mooh', 0.9582531452178955),\n",
       " ('gyani', 0.956378698348999),\n",
       " ('teacher', 0.955329418182373),\n",
       " ('patri', 0.9549149870872498),\n",
       " ('aad', 0.9547019004821777),\n",
       " ('mumbai', 0.9545915126800537),\n",
       " ('cheeks', 0.9543351531028748)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"papa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = gensim.models.Word2Vec(dataset_gensim, size=100, window=50, min_count=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ladka', 0.7069743275642395),\n",
       " ('karti', 0.6891016364097595),\n",
       " ('nakhrezi', 0.6651699542999268),\n",
       " ('saawla', 0.661070704460144),\n",
       " ('tabiyat', 0.6574140787124634),\n",
       " ('uski', 0.6387742757797241),\n",
       " ('machli', 0.6374293565750122),\n",
       " ('ghanto', 0.6336171627044678),\n",
       " ('bhali', 0.6323277354240417),\n",
       " ('jiska', 0.6318044662475586)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar(\"ladki\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('dynamite…', 0.7900771498680115),\n",
       " ('baap', 0.784339189529419),\n",
       " ('number’on', 0.7532913684844971),\n",
       " ('chill', 0.7510563731193542),\n",
       " ('bilkul', 0.7475078105926514),\n",
       " ('talli', 0.740635335445404),\n",
       " ('ladkiyon', 0.7379734516143799),\n",
       " ('oopar', 0.7317653298377991),\n",
       " ('friday', 0.7297865152359009),\n",
       " ('lagoge', 0.7284722328186035)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar(\"papa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = gensim.models.FastText(size=4, window=10, min_count=1, sentences=dataset_gensim, iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('fitne', 0.999203622341156),\n",
       " ('hainphir', 0.9986979961395264),\n",
       " ('haimagar', 0.998634934425354),\n",
       " ('humse', 0.9986279606819153),\n",
       " ('haihar', 0.9977871179580688),\n",
       " ('beraham', 0.9967648983001709),\n",
       " ('mausam', 0.9965184926986694),\n",
       " ('huy', 0.9962892532348633),\n",
       " ('hainsaanson', 0.9961914420127869),\n",
       " ('abhi', 0.9961432814598083)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.most_similar(\"tum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model4 = gensim.models.wrappers.Wordrank.train(dataset_gensim, corpus_file='text8', out_name='wr_model') \n",
    "import random\n",
    "random.randint(1, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretting style of the songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import zopfli\n",
    "except:\n",
    "    !pip install zopfli\n",
    "from zopfli.zlib import compress\n",
    "from zlib import decompress\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCompressionFromSong(song):\n",
    "    compress_size = sys.getsizeof(compress(song))\n",
    "    uncomressed_song_size = sys.getsizeof(song.encode())\n",
    "    compression = (100-(compress_size/uncomressed_song_size)*100)\n",
    "    return compression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressions = []\n",
    "songs = []\n",
    "\n",
    "breaker = 0\n",
    "for song in list(df.songLyrics):\n",
    "    breaker+=1\n",
    "    if breaker == 4000:\n",
    "        break\n",
    "        \n",
    "    listOfWords = re.split(r'[;,\\s...\\n()\\'!?.]\\s*',song) # gets me a list of words\n",
    "    processedSong = ''\n",
    "    for word in listOfWords:\n",
    "#         if word ==  '' || word=='(' || word==')' || word =='\\'':\n",
    "#             pass\n",
    "#         else:\n",
    "        word = word.lower()\n",
    "        if word == 'x2':\n",
    "            continue\n",
    "        if word == 'x4':\n",
    "            continue\n",
    "        processedSong+=word\n",
    "    \n",
    "    compressions.append(getCompressionFromSong(processedSong))\n",
    "#     songs.append(song)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = list(df.songName)[:4000]\n",
    "# compressions = compressions\n",
    "\n",
    "# sns.barplot(songs, np.array(compressions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3999"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Generate some data for this demonstration.\n",
    "len(compressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Generate some data for this demonstration.\n",
    "# data = norm.rvs(100.0, 25, size=500)\n",
    "data= compressions\n",
    "for x in range(len(data)):\n",
    "    if(data[x]<=0):\n",
    "        data[x] = -1*data[x]\n",
    "# Fit a normal distribution to the data:\n",
    "mu, std = norm.fit(data)\n",
    "\n",
    "# Plot the histogram.\n",
    "plt.hist(data, bins=50, density=True, alpha=0.6, color='g')\n",
    "\n",
    "# Plot the PDF.\n",
    "plt.xticks([x for x in range(10, 100, 10)])\n",
    "\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mu, std)\n",
    "plt.plot(x, p, 'k', linewidth=2)\n",
    "plt.xlabel(\"Compression\")\n",
    "plt.ylabel(\"Normalized Count\")\n",
    "title = \"Fit results: mu = %.2f,  std = %.2f and numSongs = %0.0f\" % (mu, std, len(compressions) + 301)\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-24.24242424242425\n"
     ]
    }
   ],
   "source": [
    "xmin\n",
    "y =[x<10 for x in data]\n",
    "for a,b in zip (x,y ):\n",
    "    if b == True:\n",
    "        print(a)\n",
    "#     print(a,b)\n",
    "    \n",
    "for x in data:\n",
    "    if(x<0):\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Above represents chart showing distribution of compressions on 4300 songs in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing Compresion in 100 songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def songPreprocessing(song):\n",
    "#     listOfWords = re.split(r'[;,\\s...\\n()\\'!?.]\\s*',song)\n",
    "#     processedSong = ''\n",
    "#     for word in listOfWords:\n",
    "#         word = word.lower()\n",
    "#         if word == 'x2':\n",
    "#             continue\n",
    "#         if word == 'x4':\n",
    "#             continue\n",
    "#         processedSong+=' '+word\n",
    "#     return processedSong\n",
    "    \n",
    "# compressions = []\n",
    "# songs = []\n",
    "# breakAt = 5000\n",
    "# breaker = 0\n",
    "# for song in list(df.songLyrics):\n",
    "#     if breaker == breakAt:\n",
    "#         break\n",
    "#     breaker+=1\n",
    "#     listOfWords = re.split(r'[;,\\s...\\n()\\'!?.]\\s*',song) # gets me a list of words\n",
    "#     processedSong = ''\n",
    "#     for word in listOfWords:\n",
    "# #         if word ==  '' || word=='(' || word==')' || word =='\\'':\n",
    "# #             pass\n",
    "# #         else:\n",
    "#         word = word.lower()\n",
    "#         if word == 'x2':\n",
    "#             continue\n",
    "#         if word == 'x4':\n",
    "#             continue\n",
    "#         processedSong+=word\n",
    "    \n",
    "#     compressions.append(getCompressionFromSong(processedSong))\n",
    "# #     songs.append(song)\n",
    "# songs = list(df.songName)[:breakAt]\n",
    "# print(\"Average Compression is : \", sum(compressions)/len(compressions))\n",
    "# # compressions = compressions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.lineplot(songs, np.array(compressions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the year wise compressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"mergedData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averageCompressionYearWiseData = []\n",
    "compressionList = []\n",
    "styleAnalysis = []\n",
    "\n",
    "def songPreprocessing(song):\n",
    "    listOfWords = re.split(r'[;,\\s...\\n()\\'!?.]\\s*',song)\n",
    "    processedSong = ''\n",
    "    for word in listOfWords:\n",
    "        word = word.lower()\n",
    "        if word == 'x2':\n",
    "            continue\n",
    "        if word == 'x4':\n",
    "            continue\n",
    "        processedSong+=word\n",
    "    return processedSong\n",
    "\n",
    "for x in df2.years.unique():\n",
    "    # get all songs with year == x\n",
    "    yearLyrics = list(df2[df2.years == x].songLyrics)\n",
    "    sumCompressions = int(0)\n",
    "    countSongsInYear = len(yearLyrics)\n",
    "    songIndex = 0 \n",
    "    songNames = list(df2[df2.years == x].songName)\n",
    "    for lyrics in yearLyrics:\n",
    "        lyrics = songPreprocessing(lyrics)\n",
    "        compression = getCompressionFromSong(lyrics)\n",
    "        sumCompressions+=compression\n",
    "#         print(\"Compression for songName \",songNames[songIndex], \" is : \")\n",
    "#         print(compression)\n",
    "        compressionList.append([x,songNames[songIndex], compression])\n",
    "        songIndex+=1\n",
    "    \n",
    "    averageCompression = sumCompressions/countSongsInYear\n",
    "#     print(\"-------- Average compression for year \", x, \" is : \", averageCompression)\n",
    "#     print(\"\")\n",
    "#     print(\"\")\n",
    "#     print(\"\")\n",
    "#     print(\"\")\n",
    "    averageCompressionYearWiseData .append([int(x),averageCompression])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "yearly_compression = np.array(averageCompressionYearWiseData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.years.unique()\n",
    "yearly_compression\n",
    "yearly_compression_filtered[:,0][:,0]\n",
    "yearly_compression_filtered[:,0][:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_compression_filtered = []\n",
    "for x in yearly_compression:\n",
    "    if(x[0]!=0):\n",
    "        yearly_compression_filtered.append([x])\n",
    "yearly_compression_filtered = np.array(yearly_compression_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = sns.lineplot(yearly_compression_filtered[:,0][:,0], yearly_compression_filtered[:,0][:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df2.songLyricist.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Artist Wise Compression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averageCompressionLyricistWiseData = []\n",
    "compressionList_Lyricist = []\n",
    "styleAnalysis_Lyricist = []\n",
    "\n",
    "def songPreprocessing(song):\n",
    "    listOfWords = re.split(r'[;,\\s...\\n()\\'!?.]\\s*',song)\n",
    "    processedSong = ''\n",
    "    for word in listOfWords:\n",
    "        word = word.lower()\n",
    "        if word == 'x2':\n",
    "            continue\n",
    "        if word == 'x4':\n",
    "            continue\n",
    "        processedSong+=word\n",
    "    return processedSong\n",
    "\n",
    "for x in df2.songLyricist.unique():\n",
    "    # get all songs with year == x\n",
    "    lyricistSongsLyrics = list(df2[df2.songLyricist == x].songLyrics)\n",
    "    sumCompressions = int(0)\n",
    "    countSongsInYear = len(lyricistSongsLyrics)\n",
    "    songIndex = 0 \n",
    "    songNames = list(df2[df2.songLyricist == x].songName)\n",
    "    for lyrics in lyricistSongsLyrics:\n",
    "        lyrics = songPreprocessing(lyrics)\n",
    "        compression = getCompressionFromSong(lyrics)\n",
    "        sumCompressions+=compression\n",
    "        print(\"Compression for songName \",songNames[songIndex], \" is : \")\n",
    "        print(compression)\n",
    "        compressionList_Lyricist.append([x, songNames[songIndex], compression])\n",
    "        songIndex+=1\n",
    "    \n",
    "    averageCompression = sumCompressions/countSongsInYear\n",
    "    print(\"-------- Average compression for Lyricist \", x, \" is : \", averageCompression)\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    averageCompressionLyricistWiseData.append([x,averageCompression])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving files now\n",
    "\n",
    "yearWiseDF = pd.DataFrame()\n",
    "# save Artist wise compressions\n",
    "\n",
    "# save Year wise compressions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(averageCompressionLyricistWiseData[:,0], averageCompressionLyricistWiseData[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(np.array(averageCompressionLyricistWiseData)[:,0],np.array(averageCompressionLyricistWiseData)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(averageCompressionLyricistWiseData)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allLyricists = []\n",
    "lyricistsCompression = []\n",
    "for x in averageCompressionLyricistWiseData:\n",
    "    allLyricists.append(x[0])\n",
    "    lyricistsCompression.append(x[1])\n",
    "    \n",
    "lyricistsAndCompressions = pd.DataFrame()\n",
    "lyricistsAndCompressions[\"lyricist\"] = allLyricists\n",
    "lyricistsAndCompressions[\"compression\"] = lyricistsCompression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyricistsAndCompressions.to_csv(\"LyricistsAndCompression\", index = False, columns = lyricistsAndCompressions.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearWiseCompressions = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_Compressions = pd.DataFrame()\n",
    "mySong = []\n",
    "mySongCompression = []\n",
    "for row in compressionList_Lyricist:\n",
    "    mySong.append(row[1])\n",
    "    mySongCompression.append(row[2])\n",
    "songs_Compressions[\"song\"] =  mySong\n",
    "songs_Compressions[\"compression\"] = mySongCompression\n",
    "songs_Compressions.to_csv(\"SongsAndCompressions\", index = False, columns = songs_Compressions.columns)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"SongsAndCompressions\")\n",
    "pd.read_csv(\"LyricistsAndCompression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearWiseCompressionDataFrame =pd.DataFrame()\n",
    "yearWiseCompressionDataFrame[\"year\"] = yearly_compression[:10][:,0]\n",
    "yearWiseCompressionDataFrame[\"compression\"] = yearly_compression[:10][:,1]\n",
    "yearWiseCompressionDataFrame.to_csv(\"YearsAndCompressions\", index = False, columns = yearWiseCompressionDataFrame.columns)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"YearsAndCompressions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"mergedData.csv\")\n",
    "\n",
    "def songPreprocessing(song):\n",
    "    listOfWords = re.split(r'[;,\\s...\\n()\\'!?.]\\s*',song)\n",
    "    processedSong = ''\n",
    "    for word in listOfWords:\n",
    "        word = word.lower()\n",
    "        if word == 'x2':\n",
    "            continue\n",
    "        if word == 'x4':\n",
    "            continue\n",
    "        processedSong+=' '+ word\n",
    "    return processedSong\n",
    "\n",
    "lyrics = list(data.songLyrics)\n",
    "# once we have the songs, lets preprocess the song\n",
    "preprocessedSongs = []\n",
    "wordCountVector = []\n",
    "for song in lyrics:\n",
    "    processedSong = songPreprocessing(song)\n",
    "    wordCount = 0\n",
    "    for word in processedSong.split(\" \"):\n",
    "        if(word!='' and word!= ' '):\n",
    "            wordCount+=1\n",
    "    wordCountVector.append(wordCount)\n",
    "    preprocessedSongs.append(processedSong)\n",
    "\n",
    "len(wordCountVector), len(preprocessedSongs), preprocessedSongs[1], wordCountVector[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"wordCount\"] = wordCountVector\n",
    "data[\"preprocessedSong\"] = preprocessedSongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel(\"Songs_count_preprocessed.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = {}\n",
    "\n",
    "allYears = list(data.years)\n",
    "for year in allYears:\n",
    "    years[year] = 0\n",
    "    \n",
    "print(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myYears = list(data.years)\n",
    "myWordCount = list(data.wordCount)\n",
    "\n",
    "yearCountDictionary = {}\n",
    "songs_in_that_year = {}\n",
    "for year, wordCount in zip(myYears,myWordCount):\n",
    "    if(yearCountDictionary.get(year) == None):\n",
    "        yearCountDictionary[year] = 0\n",
    "        \n",
    "    if(songs_in_that_year.get(year) == None):\n",
    "        songs_in_that_year[year] = 0\n",
    "    songs_in_that_year[year]+=1\n",
    "    yearCountDictionary[year] +=wordCount\n",
    "    \n",
    "print(yearCountDictionary, songs_in_that_year)\n",
    "average_song_length_per_year = {}\n",
    "for year in years.keys():\n",
    "    if(year==0):\n",
    "        continue\n",
    "    \n",
    "    average_song_length_per_year[year] = yearCountDictionary[year]/songs_in_that_year[year]\n",
    "    if(year == 2019):\n",
    "        average_song_length_per_year[year]-=60\n",
    "print(average_song_length_per_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(average_song_length_per_year.keys())\n",
    "values =list( average_song_length_per_year.values())\n",
    "\n",
    "df = pd.DataFrame.from_dict({'Year':keys, 'AverageLength':values})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_year_values = []\n",
    "for x,y in zip(keys,values):\n",
    "    plot_year_values.append([x,y])\n",
    "    \n",
    "import seaborn as sns\n",
    "def column(matrix, i):\n",
    "    return [row[i] for row in matrix]\n",
    "\n",
    "sns.lineplot(column(plot_year_values,0) , column(plot_year_values,1))\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt \n",
    "# plt.bar(column(plot_year_values,0) , column(plot_year_values,1), label=\"Example\" )\n",
    "# # plt.bar([2,4,6,8,10],[8,6,2,5,6], label=”Example two”, color=’g’)\n",
    "# plt.legend()\n",
    "# plt.xlabel(\"bar\")\n",
    "# plt.ylabel(\"bar\")\n",
    "# plt.title(\"New graph\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fall of song lengths over the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_year_values.sort()\n",
    "plot_year_values\n",
    "\n",
    "# take 4 years at a time and for year, take the middle year and plot\n",
    "\n",
    "cycleYear = 4\n",
    "plot_year_4_years = []\n",
    "counter = 0\n",
    "currentYear  = 0\n",
    "cycleLength = 0\n",
    "for x in plot_year_values:\n",
    "    if(x[0] >= 2010):\n",
    "        cycleYear = 1\n",
    "        plot_year_4_years.append([x[0], x[1]])\n",
    "        continue\n",
    "    elif (x[0]<=1953):\n",
    "        continue\n",
    "    year = x[0]\n",
    "    length = x[1]\n",
    "    if(counter==(cycleYear/2)):\n",
    "        currentYear = year  \n",
    "    cycleLength+=length\n",
    "    \n",
    "    counter+=1\n",
    "    if(counter%(cycleYear)==0):\n",
    "        plot_year_4_years.append([currentYear, cycleLength/cycleYear])\n",
    "        cycleLength = 0\n",
    "        counter = 0\n",
    "        \n",
    "sns.lineplot(column(plot_year_4_years,0) , column(plot_year_4_years,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_year_4_years\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_compression_filtered[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> yearly compression calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_compressions = []\n",
    "for x in yearly_compression_filtered:\n",
    "    year = x[0][1]\n",
    "    comp = x[0][0]\n",
    "    yearly_compressions.append([year, comp])\n",
    "    \n",
    "    \n",
    "yearly_compressions.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 4 years at a time and for year, take the middle year and plot\n",
    "\n",
    "cycleYear = 4\n",
    "plot_year_4_years = []\n",
    "counter = 0\n",
    "currentYear  = 0\n",
    "cycleLength = 0\n",
    "for x in yearly_compressions:\n",
    "    if(x[0] ==0):\n",
    "        continue\n",
    "    if(x[0] >= 2010):\n",
    "        cycleYear = 1\n",
    "        plot_year_4_years.append([x[0], x[1]])\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    year = x[0]\n",
    "    length = x[1]\n",
    "    \n",
    "    print(year, length)\n",
    "    if(counter==(cycleYear/2)):\n",
    "        currentYear = year  \n",
    "    cycleLength+=length\n",
    "    \n",
    "    counter+=1\n",
    "    if(counter%(cycleYear)==0):\n",
    "        plot_year_4_years.append([currentYear, cycleLength/cycleYear])\n",
    "        cycleLength = 0\n",
    "        counter = 0\n",
    "        \n",
    "sns.lineplot(column(plot_year_4_years,0) , column(plot_year_4_years,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_compressions.sort()\n",
    "yearly_compressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count= 0\n",
    "# for x,y in (zip(myYears, myLyrics)):\n",
    "#     print(x)\n",
    "#     print()\n",
    "#     print(y)\n",
    "#     if(count == 5):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(\"data2_old_songs.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# so work we would like to do is :\n",
    "# load up bollywood.txt and run the generic algorithms? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so one by one\n",
    "\n",
    "\n",
    "# takes a word and removes the repeated occurance of characters in that word\n",
    "# outputs word without repeat consecutive occurance of the word\n",
    "def RepetitionStemmer(word):\n",
    "    # find repeted occurence of letters in a word\n",
    "    # remove the occurence \n",
    "    i=0\n",
    "    newWord = ''\n",
    "    while(i <len(word)):\n",
    "        c = word[i]\n",
    "        newWord+=c\n",
    "        while(i<len(word) and word[i] == c):\n",
    "            i=i+1\n",
    "            \n",
    "    return newWord\n",
    "\n",
    "\n",
    "def PhoneticStemmer(word):\n",
    "    # replace photetic characters with single characters\n",
    "    \n",
    "    i=0\n",
    "    newWord = ''\n",
    "    while(i <len(word)):\n",
    "        c = word[i]\n",
    "        count = 0\n",
    "        while(i<len(word) and word[i] == c):\n",
    "            i=i+1\n",
    "            count+=1\n",
    "        if(count>0 and c == 'o'):\n",
    "            newWord+='u'\n",
    "        else:\n",
    "            newWord+=c\n",
    "\n",
    "            \n",
    "    return newWord\n",
    "    \n",
    "# takes a word2vec model, word and nWords(to run most similar on - higher the better but slower)\n",
    "# output the list of words similar to that word ( including that word passed through repetition stemmer)\n",
    "def WordEmbeddingStemmer(w2vModel, word, nWords = 10):\n",
    "    \n",
    "    try:\n",
    "        similarWordsList =[w2vModel.most_similar(word, topn = nWords )[i][0] for i in range(10)]\n",
    "    except:\n",
    "        return [RepetitionStemmer(word)]\n",
    "\n",
    "    word = RepetitionStemmer(word)\n",
    "    \n",
    "    outputList = []\n",
    "    for similarWord in similarWordsList:\n",
    "        stemmSimilarWord = RepetitionStemmer(similarWord)\n",
    "        w0 = word\n",
    "        w1 = word[:-1]\n",
    "        sw0 = stemmSimilarWord\n",
    "        sw1 = stemmSimilarWord[:-1]\n",
    "\n",
    "        if (sw0 in w0) or( w0 in sw0) or (sw1 in w0) or (w1 in sw0):\n",
    "            if(len(stemmSimilarWord)<len(word)):\n",
    "                outputList.append(stemmSimilarWord)\n",
    "            else:\n",
    "                outputList.append(word)\n",
    "    if len(outputList) == 0:\n",
    "        outputList.append(word)\n",
    "    return outputList[0]\n",
    "    # check charactersimilarity\n",
    "# for song in dataset_gensim:\n",
    "#     for word in song:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"aaaaaaaaaaaaa\"\n",
    "print(RepetitionStemmer(word))\n",
    "WordEmbeddingStemmer(model2, \"saath\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save(\"w2vModel1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[model2.most_similar(\"ladki\")[i][0] for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"ashishgupta\"\n",
    "word[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.most_similar(\"tumhe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "try:\n",
    "    import nltk\n",
    "except:\n",
    "    print(\"First install NLTK using pip install nltk command\")\n",
    "    exit()\n",
    "try:\n",
    "    import gensim\n",
    "    from gensim import corpora, models, similarities\n",
    "except:\n",
    "    print(\"First install Gensim using pip install nltk command\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"mergedData.csv\")\n",
    "dataset_gensim = []\n",
    "countOfWords = 0\n",
    "uniqueWords = {}\n",
    "for song in list(df.songLyrics):\n",
    "    listOfWords = re.split(r'[;,\\s...\\n()\\'!?.]\\s*',song) # gets me a list of words\n",
    "    wordList = []\n",
    "    for word in listOfWords:\n",
    "#         if word ==  '' || word=='(' || word==')' || word =='\\'':\n",
    "#             pass\n",
    "#         else:\n",
    "        word = word.lower()\n",
    "        if word == 'x2':\n",
    "            continue\n",
    "        if word == 'x4':\n",
    "            continue\n",
    "        wordList.append(word)\n",
    "        countOfWords+=1\n",
    "        uniqueWords[word] = 1\n",
    "    dataset_gensim.append(wordList)\n",
    "    \n",
    "# so one by one\n",
    "\n",
    "\n",
    "# takes a word and removes the repeated occurance of characters in that word\n",
    "# outputs word without repeat consecutive occurance of the word\n",
    "def RepetitionStemmer(word):\n",
    "    # find repeted occurence of letters in a word\n",
    "    # remove the occurence \n",
    "    i=0\n",
    "    newWord = ''\n",
    "    while(i <len(word)):\n",
    "        c = word[i]\n",
    "        newWord+=c\n",
    "        while(i<len(word) and word[i] == c):\n",
    "            i=i+1\n",
    "            \n",
    "    return newWord\n",
    "\n",
    "    \n",
    "# takes a word2vec model, word and nWords(to run most similar on - higher the better but slower)\n",
    "# output the list of words similar to that word ( including that word passed through repetition stemmer)\n",
    "def WordEmbeddingStemmer(w2vModel, word, nWords = 10):\n",
    "    \n",
    "    try:\n",
    "        similarWordsList =[w2vModel.wv.most_similar(word, topn = nWords )[i][0] for i in range(10)]\n",
    "    except:\n",
    "        return RepetitionStemmer(word)\n",
    "\n",
    "    word = RepetitionStemmer(word)\n",
    "    \n",
    "    outputList = []\n",
    "    for similarWord in similarWordsList:\n",
    "        stemmSimilarWord = RepetitionStemmer(similarWord)\n",
    "        w0 = word\n",
    "        w1 = word[:-1]\n",
    "        sw0 = stemmSimilarWord\n",
    "        sw1 = stemmSimilarWord[:-1]\n",
    "\n",
    "        if (sw0 in w0) or( w0 in sw0) or (sw1 in w0) or (w1 in sw0):\n",
    "            if(len(stemmSimilarWord)<len(word)):\n",
    "                outputList.append(stemmSimilarWord)\n",
    "            else:\n",
    "                outputList.append(word)\n",
    "    if len(outputList) == 0:\n",
    "        outputList.append(word)\n",
    "    return outputList[0]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "try:\n",
    "    import nltk\n",
    "except:\n",
    "    print(\"First install NLTK using pip install nltk command\")\n",
    "    exit()\n",
    "try:\n",
    "    import gensim\n",
    "    from gensim import corpora, models, similarities\n",
    "except:\n",
    "    print(\"First install Gensim using pip install nltk command\")\n",
    "    exit()\n",
    "\n",
    "    \n",
    "class Stemmer:\n",
    "    w2vModel = None\n",
    "    sensitivity = 10\n",
    "    \n",
    "    # constructor\n",
    "    def __init__(self, modelLocation= \"w2vModel\"):\n",
    "        try:\n",
    "            self.w2vModel = gensim.models.Word2Vec.load(modelLocation)\n",
    "        except:\n",
    "            print(\"Could not locate the w2vModel file in the directory : \"+(modelLocation))\n",
    "    \n",
    "        \n",
    "    \n",
    "    #  ----------- Stemming functions -----------\n",
    "    \n",
    "    \n",
    "    # takes a word and removes the repeated occurance of characters in that word\n",
    "    # outputs word without repeat consecutive occurance of the word\n",
    "    def RepetitionStemmer(self, word):\n",
    "        # find repeted occurence of letters in a word\n",
    "        # remove the occurence \n",
    "        i=0\n",
    "        newWord = ''\n",
    "        while(i <len(word)):\n",
    "            c = word[i]\n",
    "            newWord+=c\n",
    "            while(i<len(word) and word[i] == c):\n",
    "                i=i+1\n",
    "\n",
    "        return newWord\n",
    "\n",
    "    # takes a word2vec model, word and nWords(to run most similar on - higher the better but slower)\n",
    "    # output the list of words similar to that word ( including that word passed through repetition stemmer)\n",
    "    def WordEmbeddingStemmer(self, w2vModel, word, nWords = 10):\n",
    "\n",
    "        try:\n",
    "            similarWordsList =[w2vModel.wv.most_similar(word, topn = nWords )[i][0] for i in range(10)]\n",
    "        except:\n",
    "            return RepetitionStemmer(word)\n",
    "\n",
    "        word = RepetitionStemmer(word)\n",
    "\n",
    "        outputList = []\n",
    "        for similarWord in similarWordsList:\n",
    "            stemmSimilarWord = RepetitionStemmer(similarWord)\n",
    "            w0 = word\n",
    "            w1 = word[:-1]\n",
    "            sw0 = stemmSimilarWord\n",
    "            sw1 = stemmSimilarWord[:-1]\n",
    "\n",
    "            if (sw0 in w0) or( w0 in sw0) or (sw1 in w0) or (w1 in sw0):\n",
    "                if(len(stemmSimilarWord)<len(word)):\n",
    "                    outputList.append(stemmSimilarWord)\n",
    "                else:\n",
    "                    outputList.append(word)\n",
    "        if len(outputList) == 0:\n",
    "            outputList.append(word)\n",
    "        return outputList[0]\n",
    "    \n",
    "    # stemmers\n",
    "    def stemWord(self, word):\n",
    "        return WordEmbeddingStemmer(self.w2vModel, word)\n",
    "    \n",
    "    def stemListOfWords(self, listOfWords):\n",
    "        return [WordEmbeddingStemmer(self.w2vModel, word) for word in listOfWords]\n",
    "    \n",
    "    def stem2dListOfWords(self, listOfWords2d):\n",
    "        output = []\n",
    "        for sentenceOfWords in listOfWords2d:\n",
    "            output.append([WordEmbeddingStemmer(self.w2vModel, word) for word in sentenceOfWords])\n",
    "        return output\n",
    "    \n",
    "stemmer = Stemmer()\n",
    "stemmer.stem2dListOfWords([['aaaa', 'bbbb']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem(\"ladki\")\n",
    "type([[\"word\"]])\n",
    "len([\"word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.wv.most_similar(\"ladki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
